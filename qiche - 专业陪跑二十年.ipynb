{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_2.csv')\n",
    "test = pd.read_csv('../data/test_public_2v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subject = list(train['subject'].unique())\n",
    "def get_subject(x):\n",
    "    for i in range(len(subject)):\n",
    "        if x==subject[i]:\n",
    "            return i\n",
    "    return -1\n",
    "train['Y1'] = train['subject'].apply(get_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_value = list(train['sentiment_value'].unique())\n",
    "def get_sentiment_value(x):\n",
    "    for i in range(len(sentiment_value)):\n",
    "        if x==sentiment_value[i]:\n",
    "            return i\n",
    "    return -1\n",
    "train['Y2'] = train['sentiment_value'].apply(get_sentiment_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#合并主题和感情，共三十类\n",
    "train['Y3'] = train['Y1']*3+train['Y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12572, 8) 10654\n"
     ]
    }
   ],
   "source": [
    "print (train.shape,train['content_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      content_id  counts\n",
      "505          875       7\n",
      "3846        6330       6\n",
      "8601       14078       6\n",
      "4304        7033       6\n",
      "5250        8614       5\n"
     ]
    }
   ],
   "source": [
    "gp = train[['content_id']].groupby(['content_id']).size().rename('counts').reset_index()\n",
    "gp = gp.sort_values(by='counts',ascending=False)\n",
    "print (gp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1472, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp[gp['counts']>1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10654, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#合并重复数据，获取多标签数据\n",
    "def get_ys(x):\n",
    "    x = list(x)\n",
    "    ans = np.zeros(30)\n",
    "    for i in x:\n",
    "        ans[i]=1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpy = train.groupby(['content_id'])['Y3'].apply(get_ys).rename('Y4').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10654, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1 = train.groupby(['content_id','content']).size().rename('counts').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train0 = pd.merge(gpy,train1[['content_id','content']],on=['content_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10654, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau,Callback\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers.core import Layer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 635793 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# 使用预训练的词向量和字向量\n",
    "# https://github.com/Embedding/Chinese-Word-Vectors\n",
    "embeddings_index = {}\n",
    "EMBEDDING_DIM = 300\n",
    "embfile = '../data/sgns.baidubaike.bigram-char'\n",
    "with open(embfile, encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        words = values[:-EMBEDDING_DIM]\n",
    "        word = ''.join(words)\n",
    "        try:\n",
    "            coefs = np.asarray(values[-EMBEDDING_DIM:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            pass\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#分词\n",
    "rls = ['？','！','“','”','：','…','（','）',\n",
    "      '—','《','》','、','‘','’','\"','\\n','.',\n",
    "       '；','#','【','】','\\'',':','(','」','∠','+',',',\n",
    "       '!','|',\n",
    "      ]\n",
    "def cut_words(x):\n",
    "    x = str(x).strip()\n",
    "    for c in rls:\n",
    "        x = x.replace(c,' ')\n",
    "    x = ' '.join(x.split())\n",
    "    s = ' '.join(jieba.cut(x,cut_all=True))\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分字\n",
    "def cut_chars(x):\n",
    "    x = str(x).replace(' ','')\n",
    "    y = [i for i in x]\n",
    "    y = ' '.join(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['chars'] = train['content'].apply(cut_chars)\n",
    "test['chars'] = test['content'].apply(cut_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    10654.000000\n",
      "mean        44.624273\n",
      "std         31.318177\n",
      "min          9.000000\n",
      "25%         22.000000\n",
      "50%         36.000000\n",
      "75%         57.000000\n",
      "max        200.000000\n",
      "Name: chars_len, dtype: float64\n",
      "count    2753.000000\n",
      "mean       43.404286\n",
      "std        30.659615\n",
      "min         9.000000\n",
      "25%        21.000000\n",
      "50%        35.000000\n",
      "75%        55.000000\n",
      "max       199.000000\n",
      "Name: chars_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train['chars_len'] = train['chars'].apply(lambda x:len(x.split()))\n",
    "test['chars_len'] = test['chars'].apply(lambda x:len(x.split()))\n",
    "print (train['chars_len'].describe())\n",
    "print (test['chars_len'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\CHANTC~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.636 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "train['words'] = train['content'].apply(cut_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    建议 议定 定做 地毯 还有 后尾 箱 的 垫 前后 后尾 箱 大概 650 700 元 主...\n",
       "1                            2 5 的 综合 油耗 好像 普遍 在 10 左右\n",
       "2                                   确实 该 检查 了 油耗 太 高 了\n",
       "3    5 3 万公里 公里 目前 什么 都 没有 换 异 响 还好 2 0L 自 吸 8 4 油耗...\n",
       "4                       SE 2 5T 的 是 EJ 不是 FB 发动 发动机 动机\n",
       "5    我 试过 过晚 晚上 开 错 路 去 了 山间 小路 荒无人烟 无人 人烟 突然 导航 来 ...\n",
       "6    北京 石景 石景山 景山 古城 的 那个 店 丰台 有 一个 也 这个 价格 置换 还有 4...\n",
       "7                                  油耗 没 那么 高 吧 10 个 左右\n",
       "8    16 款 2 5 春节 高速 一个 CRV 不服 我 以 170 180 奔跑 数分 数分钟...\n",
       "9    优点 省油 全 时 四 驱 空间 缺点 车 漆 薄 高速 烧 机油 保养 贵 我 的 车 三...\n",
       "Name: words, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['words'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10654/10654 [00:00<00:00, 43002.56it/s]\n"
     ]
    }
   ],
   "source": [
    "#去除低频词\n",
    "word_count1 = {}\n",
    "word_count2 = {}\n",
    "for i in tqdm(range(len(train))):\n",
    "    td = {}\n",
    "    s = train.loc[i,'words'].split()\n",
    "    for c in s:\n",
    "        if c not in word_count1:\n",
    "            word_count1[c]=1\n",
    "        else:\n",
    "            word_count1[c]+=1\n",
    "        if c not in td:\n",
    "            td[c] = 1\n",
    "    for c in td:\n",
    "        if c not in word_count2:\n",
    "            word_count2[c]=1\n",
    "        else:\n",
    "            word_count2[c]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_low_words(x):\n",
    "    s = x.split()\n",
    "    t = []\n",
    "    for c in s:\n",
    "        if c in word_count1 and c in word_count2 and word_count1[c]>1 and word_count2[c]>1:\n",
    "            t.append(c)\n",
    "    return ' '.join(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['words1'] = train['words'].apply(remove_low_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    建议 议定 定做 地毯 还有 后尾 箱 的 垫 前后 后尾 箱 大概 650 700 元 主...\n",
       "1                            2 5 的 综合 油耗 好像 普遍 在 10 左右\n",
       "2                                   确实 该 检查 了 油耗 太 高 了\n",
       "3    5 3 万公里 公里 目前 什么 都 没有 换 异 响 还好 2 0L 自 吸 8 4 油耗...\n",
       "4                       SE 2 5T 的 是 EJ 不是 FB 发动 发动机 动机\n",
       "Name: words1, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['words1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['words_len'] = train['words1'].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    10654.000000\n",
      "mean        26.570021\n",
      "std         18.490809\n",
      "min          3.000000\n",
      "25%         13.000000\n",
      "50%         22.000000\n",
      "75%         34.000000\n",
      "max        127.000000\n",
      "Name: words_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print (train['words_len'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2753.000000\n",
      "mean       25.388667\n",
      "std        17.658873\n",
      "min         4.000000\n",
      "25%        13.000000\n",
      "50%        21.000000\n",
      "75%        32.000000\n",
      "max       121.000000\n",
      "Name: words_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "test['words'] = test['content'].apply(cut_words)\n",
    "test['words1'] = test['words'].apply(remove_low_words)\n",
    "test['words_len'] = test['words1'].apply(lambda x:len(x.split()))\n",
    "print (test['words_len'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "MAX_SEQUENCE_LENGTH1 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9905\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train['words1'])\n",
    "word_index = tokenizer.word_index\n",
    "print (len(word_index))\n",
    "nb_words = min(MAX_NB_WORDS,len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_words = tokenizer.texts_to_sequences(train['words1'])\n",
    "test_words = tokenizer.texts_to_sequences(test['words1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9263 642\n"
     ]
    }
   ],
   "source": [
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "cc = 0\n",
    "cc1 = 0\n",
    "for word, i in word_index.items():\n",
    "    #print (word,tokenizer.word_counts[word])\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "        cc +=1\n",
    "    else:\n",
    "        cc1+=1\n",
    "print (cc,cc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer1.fit_on_texts(train['chars'])\n",
    "word_index1 = tokenizer1.word_index\n",
    "print (len(word_index1))\n",
    "nb_words1 = min(MAX_NB_WORDS,len(word_index1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_chars = tokenizer1.texts_to_sequences(train['chars'])\n",
    "test_chars = tokenizer1.texts_to_sequences(test['chars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793 19\n"
     ]
    }
   ],
   "source": [
    "word_embedding_matrix1 = np.zeros((nb_words1 + 1, EMBEDDING_DIM))\n",
    "cc = 0\n",
    "cc1 = 0\n",
    "for word, i in word_index1.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix1[i] = embeddings_index[word]\n",
    "        cc +=1\n",
    "    else:\n",
    "        cc1+=1\n",
    "print (cc,cc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pad_char_seq(x):\n",
    "    return pad_sequences(x,maxlen=MAX_SEQUENCE_LENGTH1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pad_seq(x):\n",
    "    return pad_sequences(x,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pad_sequences(train_words,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_X = pad_sequences(test_words,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_X1 = get_pad_char_seq(test_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10654, 128)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.array(list(train['Y4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10654, 30)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#f1_score, 总出现NAN，发现是K.sum会得到实数。。。就强行输出0\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_pred, 0, 1))*K.round(K.clip(y_true, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c1==0 or c3 ==0 or c2==0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / (c2+0.000001)\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / (c3+0.000001)\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall+0.000001)\n",
    "    return f1\n",
    "\n",
    "def c1(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_pred, 0, 1)*K.clip(y_true, 0, 1)))\n",
    "    return c1\n",
    "\n",
    "def c2(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return c2\n",
    "\n",
    "def c3(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#数据增强与采样\n",
    "np.random.seed(1992)\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, data,data1,datay,\n",
    "                 batch_size=256, shuffle=True,aug=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.data1 = data1\n",
    "        self.datay = datay\n",
    "        self.aug = aug\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = np.array(range(index*self.batch_size,(index+1)*self.batch_size))\n",
    "        indexes = indexes%len(self.data)\n",
    "        indexes = self.indexes[indexes]\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.array(range(len(self.data)))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        X = []\n",
    "        X1 = []\n",
    "        y = []\n",
    "        for i in range(self.batch_size):\n",
    "            X.append(self.data[indexes[i]])\n",
    "            X1.append(self.data1[indexes[i]])\n",
    "            y.append(self.datay[indexes[i]])\n",
    "            \n",
    "        if self.aug>0:\n",
    "            for i in range(self.aug):\n",
    "                while True:\n",
    "                    a = np.random.randint(self.batch_size)\n",
    "                    b = np.random.randint(len(self.data))\n",
    "                    a = indexes[a]\n",
    "                    #b = indexes[b]\n",
    "                    xx = self.data[a]+self.data[b]\n",
    "                    xx1 = self.data1[a]+self.data1[b]\n",
    "                    if len(xx)<MAX_SEQUENCE_LENGTH and len(xx1)<MAX_SEQUENCE_LENGTH1:\n",
    "                        yy = self.datay[a]+self.datay[b]\n",
    "                        yy = np.minimum(yy,1)\n",
    "                        X.append(xx)\n",
    "                        X1.append(xx1)\n",
    "                        y.append(yy)\n",
    "                        break;\n",
    "        X = get_pad_seq(X)   \n",
    "        X1 = get_pad_char_seq(X1) \n",
    "        y = np.array(y)\n",
    "        return [X,X1],y\n",
    "    \n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True}\n",
    "\n",
    "#training_generator = DataGenerator(X_train,y_train, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold,KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "skf = KFold(n_splits=N,shuffle=True,random_state=1337)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#注意力层\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    " \n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    " \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    " \n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    " \n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    " \n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    " \n",
    "        a = K.exp(ait)\n",
    " \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    " \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    " \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#胶囊网络\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "st_model = '../data/st_model/'\n",
    "if not os.path.exists(st_model):\n",
    "    os.mkdir(st_model)\n",
    "st_csv = '../data/st_csv/'\n",
    "if not os.path.exists(st_csv):\n",
    "    os.mkdir(st_csv)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#后面是结构微调的四个模型，第一个复赛A 0.6858，第四个复赛A 0.6825，第二个微差，第三个训崩了，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model_att0(x0,x1):\n",
    "    Embedding_layer0 = Embedding(nb_words + 1, EMBEDDING_DIM, \n",
    "                                weights=[word_embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                name = 'lc_embed0',\n",
    "                                trainable = False,\n",
    "                                )\n",
    "    Embedding_layer1 = Embedding(nb_words + 1, 100, \n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                name = 'lc_embed1',\n",
    "                                trainable = True,\n",
    "                                )\n",
    "    Embedding_layer2 = Embedding(nb_words1 + 1, EMBEDDING_DIM, \n",
    "                                weights=[word_embedding_matrix1],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH1, \n",
    "                                name = 'lc_embed2',\n",
    "                                trainable = False,\n",
    "                                )\n",
    "    Embedding_layer3 = Embedding(nb_words1 + 1, 100, \n",
    "                                input_length=MAX_SEQUENCE_LENGTH1, \n",
    "                                name = 'lc_embed3',\n",
    "                                trainable = True,\n",
    "                                )\n",
    "    \n",
    "    x = Embedding_layer0(x0)\n",
    "    x01 = Embedding_layer1(x0)\n",
    "    x2 = Embedding_layer2(x1)\n",
    "    x3 = Embedding_layer3(x1)\n",
    "    x01 = SpatialDropout1D(0.5)(x01)\n",
    "    x3 = SpatialDropout1D(0.5)(x3)\n",
    "    \n",
    "    xa0 = Concatenate(axis=2)([x,x01])\n",
    "    xb0 = Concatenate(axis=2)([x2,x3])\n",
    "        \n",
    "    xa = Bidirectional(GRU(256, return_sequences=True,dropout=0.25))(xa0)    \n",
    "    xb = Bidirectional(GRU(256, return_sequences=True,dropout=0.25))(xb0)\n",
    "\n",
    "    xa2 = AttentionWithContext()(xa)\n",
    "    xb2 = AttentionWithContext()(xb)\n",
    "\n",
    "    ya = xa2\n",
    "    yb = xb2\n",
    "    \n",
    "    ya = Dropout(0.5)(ya)\n",
    "    yb = Dropout(0.5)(yb)\n",
    "    y = Concatenate(axis=1)([ya,yb])\n",
    "    y = Dense(30, kernel_initializer='he_normal', activation='sigmoid')(y)        \n",
    "    return y\n",
    "    \n",
    "#inputxa = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "#inputxb = Input(shape=(MAX_SEQUENCE_LENGTH1,))\n",
    "#outputya = get_model_att0(inputxa,inputxb)\n",
    "#model1 = Model([inputxa,inputxb], outputya)\n",
    "#model1.summary() \n",
    "\n",
    "def get_result_att0(timei):\n",
    "    xx_cv = []\n",
    "    xx_pre = []\n",
    "    xx_train = []\n",
    "    yy_train = []\n",
    "    early_stopping = EarlyStopping(patience=3,\n",
    "                                   verbose=1,\n",
    "                                   monitor='val_f1_score',\n",
    "                                   mode='max'\n",
    "                                  )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(factor=0.1, \n",
    "                                  patience=2, \n",
    "                                  min_lr=0.00001, \n",
    "                                  verbose=1,\n",
    "                                  monitor='val_f1_score',\n",
    "                                  mode='max',\n",
    "                                 )\n",
    "    foldi = -1\n",
    "    for train_in,test_in in skf.split(train_words,Y):\n",
    "        foldi = foldi+1\n",
    "        X_train = []\n",
    "        X_traina = []\n",
    "        for i in train_in:\n",
    "            X_train.append(train_words[i])\n",
    "            X_traina.append(train.loc[i,'words1'])\n",
    "\n",
    "        X_test = []\n",
    "        for i in test_in:\n",
    "            X_test.append(train_words[i])\n",
    "\n",
    "        X_train1 = []\n",
    "        for i in train_in:\n",
    "            X_train1.append(train_chars[i])\n",
    "\n",
    "        X_test1 = []\n",
    "        for i in test_in:\n",
    "            X_test1.append(train_chars[i])\n",
    "\n",
    "        y_train,y_test = Y[train_in],Y[test_in]\n",
    "\n",
    "        X_test = get_pad_seq(X_test)\n",
    "        X_test1 = get_pad_char_seq(X_test1)\n",
    "\n",
    "        params = {'batch_size': 64,\n",
    "                  'aug':128+64,\n",
    "                  'shuffle': True}\n",
    "\n",
    "        training_generator = DataGenerator(X_train,X_train1,y_train, **params)\n",
    "\n",
    "        inputxa = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "        inputxb = Input(shape=(MAX_SEQUENCE_LENGTH1,))\n",
    "        outputya = get_model_att0(inputxa,inputxb)\n",
    "        model1 = Model([inputxa,inputxb], outputya)\n",
    "\n",
    "\n",
    "        model1.compile(loss='binary_crossentropy', \n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[f1_score],\n",
    "                     )\n",
    "        filename = st_model+str(foldi)+'_'+str(timei)+'.att0'\n",
    "        model_checkpoint = ModelCheckpoint(filename,\n",
    "                                       save_best_only=True,\n",
    "                                       verbose=1,\n",
    "                                       monitor='val_f1_score',\n",
    "                                       mode='max'\n",
    "                                      )\n",
    "            \n",
    "        history =model1.fit_generator(generator=training_generator,\n",
    "                        validation_data=[[X_test,X_test1],y_test],\n",
    "                        epochs=100,\n",
    "                        callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                       )    \n",
    "\n",
    "        model1.load_weights(filename)\n",
    "        ttesty = model1.predict([test_X,test_X1],batch_size = 128)\n",
    "        xx_pre.append(ttesty)\n",
    "        xx_cv.append(np.max(history.history['val_f1_score']))\n",
    "\n",
    "        ttesty = model1.predict([X_test,X_test1],batch_size = 128)\n",
    "        xx_train.append(ttesty)\n",
    "        yy_train.append(y_test)\n",
    "    \n",
    "    s = 0\n",
    "    for i in xx_pre:\n",
    "        s = s+i\n",
    "    s = s/5\n",
    "    \n",
    "    cols = []\n",
    "    for j in range(30):\n",
    "        cols.append(str(j))\n",
    "    res = pd.DataFrame(s,columns=cols)\n",
    "    res.to_csv(st_csv+str(foldi)+'_'+str(timei)+'_att0.csv',index=None)\n",
    "    \n",
    "    train_res = np.concatenate(xx_train, axis=0)\n",
    "    train_yy = np.concatenate(yy_train, axis=0)\n",
    "    \n",
    "    train_res = pd.DataFrame(train_res,columns=cols)\n",
    "    train_res.to_csv(st_csv+str(foldi)+'_'+str(timei)+'_att0_train.csv',index=None)\n",
    "    \n",
    "    train_yy = pd.DataFrame(train_yy,columns=cols)\n",
    "    train_yy.to_csv(st_csv+str(foldi)+'_'+str(timei)+'_att0_train_yy.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "133/133 [==============================] - 115s 866ms/step - loss: 0.1951 - f1_score: 0.3841 - val_loss: 0.0941 - val_f1_score: 0.5723\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.57235, saving model to ../data/st_model/0_3.att0\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 112s 842ms/step - loss: 0.1497 - f1_score: 0.5013 - val_loss: 0.0860 - val_f1_score: 0.5913\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.57235 to 0.59129, saving model to ../data/st_model/0_3.att0\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 113s 846ms/step - loss: 0.1365 - f1_score: 0.5337 - val_loss: 0.0824 - val_f1_score: 0.6135\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.59129 to 0.61354, saving model to ../data/st_model/0_3.att0\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 115s 864ms/step - loss: 0.1216 - f1_score: 0.5882 - val_loss: 0.0792 - val_f1_score: 0.6204\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.61354 to 0.62038, saving model to ../data/st_model/0_3.att0\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 115s 866ms/step - loss: 0.1075 - f1_score: 0.6460 - val_loss: 0.0796 - val_f1_score: 0.6201\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.62038\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 115s 864ms/step - loss: 0.0937 - f1_score: 0.7024 - val_loss: 0.0841 - val_f1_score: 0.6129\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.62038\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 116s 873ms/step - loss: 0.0807 - f1_score: 0.7493 - val_loss: 0.0835 - val_f1_score: 0.6181\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.62038\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 121s 907ms/step - loss: 0.1896 - f1_score: 0.3858 - val_loss: 0.0979 - val_f1_score: 0.5473\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.54730, saving model to ../data/st_model/1_3.att0\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 116s 874ms/step - loss: 0.1447 - f1_score: 0.5057 - val_loss: 0.0872 - val_f1_score: 0.5763\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.54730 to 0.57627, saving model to ../data/st_model/1_3.att0\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 116s 872ms/step - loss: 0.1306 - f1_score: 0.5423 - val_loss: 0.0852 - val_f1_score: 0.5983\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.57627 to 0.59828, saving model to ../data/st_model/1_3.att0\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 114s 860ms/step - loss: 0.1169 - f1_score: 0.5982 - val_loss: 0.0849 - val_f1_score: 0.6062\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.59828 to 0.60623, saving model to ../data/st_model/1_3.att0\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 116s 873ms/step - loss: 0.1010 - f1_score: 0.6662 - val_loss: 0.0880 - val_f1_score: 0.6163\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.60623 to 0.61633, saving model to ../data/st_model/1_3.att0\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 113s 846ms/step - loss: 0.0853 - f1_score: 0.7283 - val_loss: 0.0945 - val_f1_score: 0.5926\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.61633\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 112s 845ms/step - loss: 0.0734 - f1_score: 0.7772 - val_loss: 0.1033 - val_f1_score: 0.5964\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.61633\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 113s 848ms/step - loss: 0.0607 - f1_score: 0.8206 - val_loss: 0.1023 - val_f1_score: 0.5888\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.61633\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 115s 863ms/step - loss: 0.1945 - f1_score: 0.3715 - val_loss: 0.0982 - val_f1_score: 0.5766\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.57664, saving model to ../data/st_model/2_3.att0\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 111s 835ms/step - loss: 0.1464 - f1_score: 0.4975 - val_loss: 0.0868 - val_f1_score: 0.6022\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.57664 to 0.60222, saving model to ../data/st_model/2_3.att0\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 111s 835ms/step - loss: 0.1326 - f1_score: 0.5386 - val_loss: 0.0817 - val_f1_score: 0.6088\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.60222 to 0.60884, saving model to ../data/st_model/2_3.att0\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 111s 835ms/step - loss: 0.1195 - f1_score: 0.5897 - val_loss: 0.0794 - val_f1_score: 0.6104\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.60884 to 0.61036, saving model to ../data/st_model/2_3.att0\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 111s 835ms/step - loss: 0.1050 - f1_score: 0.6520 - val_loss: 0.0796 - val_f1_score: 0.6109\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.61036 to 0.61089, saving model to ../data/st_model/2_3.att0\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 111s 835ms/step - loss: 0.0914 - f1_score: 0.7064 - val_loss: 0.0834 - val_f1_score: 0.6131\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.61089 to 0.61306, saving model to ../data/st_model/2_3.att0\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 111s 836ms/step - loss: 0.0786 - f1_score: 0.7564 - val_loss: 0.0932 - val_f1_score: 0.6165\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.61306 to 0.61647, saving model to ../data/st_model/2_3.att0\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 111s 836ms/step - loss: 0.0650 - f1_score: 0.8062 - val_loss: 0.0983 - val_f1_score: 0.6119\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.61647\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 111s 835ms/step - loss: 0.0535 - f1_score: 0.8448 - val_loss: 0.1083 - val_f1_score: 0.6106\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.61647\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 111s 836ms/step - loss: 0.0435 - f1_score: 0.8786 - val_loss: 0.1109 - val_f1_score: 0.6115\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.61647\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 115s 865ms/step - loss: 0.1973 - f1_score: 0.3608 - val_loss: 0.0929 - val_f1_score: 0.5752\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.57521, saving model to ../data/st_model/3_3.att0\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 111s 831ms/step - loss: 0.1482 - f1_score: 0.4943 - val_loss: 0.0833 - val_f1_score: 0.5992\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.57521 to 0.59916, saving model to ../data/st_model/3_3.att0\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 111s 833ms/step - loss: 0.1348 - f1_score: 0.5326 - val_loss: 0.0794 - val_f1_score: 0.6248\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.59916 to 0.62482, saving model to ../data/st_model/3_3.att0\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 111s 833ms/step - loss: 0.1214 - f1_score: 0.5832 - val_loss: 0.0755 - val_f1_score: 0.6320\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.62482 to 0.63196, saving model to ../data/st_model/3_3.att0\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.1069 - f1_score: 0.6437 - val_loss: 0.0781 - val_f1_score: 0.6317\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.63196\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 111s 836ms/step - loss: 0.0939 - f1_score: 0.6963 - val_loss: 0.0821 - val_f1_score: 0.6271\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.63196\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 111s 834ms/step - loss: 0.0798 - f1_score: 0.7505 - val_loss: 0.0818 - val_f1_score: 0.6364\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.63196 to 0.63636, saving model to ../data/st_model/3_3.att0\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 111s 833ms/step - loss: 0.0766 - f1_score: 0.7628 - val_loss: 0.0823 - val_f1_score: 0.6338\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.63636\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.0751 - f1_score: 0.7708 - val_loss: 0.0833 - val_f1_score: 0.6298\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.63636\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 111s 835ms/step - loss: 0.0725 - f1_score: 0.7764 - val_loss: 0.0832 - val_f1_score: 0.6301\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.63636\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 115s 863ms/step - loss: 0.1898 - f1_score: 0.3952 - val_loss: 0.0961 - val_f1_score: 0.5675\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.56746, saving model to ../data/st_model/4_3.att0\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 111s 834ms/step - loss: 0.1493 - f1_score: 0.4951 - val_loss: 0.0883 - val_f1_score: 0.5807\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.56746 to 0.58069, saving model to ../data/st_model/4_3.att0\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 111s 833ms/step - loss: 0.1333 - f1_score: 0.5331 - val_loss: 0.0855 - val_f1_score: 0.6116\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.58069 to 0.61165, saving model to ../data/st_model/4_3.att0\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 111s 831ms/step - loss: 0.1201 - f1_score: 0.5864 - val_loss: 0.0809 - val_f1_score: 0.6151\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.61165 to 0.61514, saving model to ../data/st_model/4_3.att0\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.1049 - f1_score: 0.6510 - val_loss: 0.0807 - val_f1_score: 0.6199\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.61514 to 0.61994, saving model to ../data/st_model/4_3.att0\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 111s 831ms/step - loss: 0.0908 - f1_score: 0.7093 - val_loss: 0.0877 - val_f1_score: 0.6121\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.61994\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.0774 - f1_score: 0.7613 - val_loss: 0.0948 - val_f1_score: 0.6064\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.61994\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.0647 - f1_score: 0.8057 - val_loss: 0.0959 - val_f1_score: 0.6190\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.61994\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 115s 866ms/step - loss: 0.2035 - f1_score: 0.3555 - val_loss: 0.0956 - val_f1_score: 0.5675\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.56747, saving model to ../data/st_model/0_4.att0\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 110s 830ms/step - loss: 0.1522 - f1_score: 0.4940 - val_loss: 0.0920 - val_f1_score: 0.5949\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.56747 to 0.59485, saving model to ../data/st_model/0_4.att0\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 110s 829ms/step - loss: 0.1368 - f1_score: 0.5340 - val_loss: 0.0801 - val_f1_score: 0.6212\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.59485 to 0.62124, saving model to ../data/st_model/0_4.att0\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 110s 829ms/step - loss: 0.1215 - f1_score: 0.5846 - val_loss: 0.0785 - val_f1_score: 0.6205\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.62124\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 110s 830ms/step - loss: 0.1065 - f1_score: 0.6442 - val_loss: 0.0776 - val_f1_score: 0.6272\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.62124 to 0.62722, saving model to ../data/st_model/0_4.att0\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.0934 - f1_score: 0.6978 - val_loss: 0.0841 - val_f1_score: 0.6180\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.62722\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 111s 831ms/step - loss: 0.0810 - f1_score: 0.7448 - val_loss: 0.0871 - val_f1_score: 0.6057\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.62722\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 110s 830ms/step - loss: 0.0675 - f1_score: 0.7958 - val_loss: 0.0905 - val_f1_score: 0.6221\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.62722\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 115s 863ms/step - loss: 0.1897 - f1_score: 0.3954 - val_loss: 0.0975 - val_f1_score: 0.5455\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.54554, saving model to ../data/st_model/1_4.att0\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 110s 826ms/step - loss: 9.5759 - f1_score: 0.2579 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.54554\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 110s 824ms/step - loss: 14.8944 - f1_score: 0.1234 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.54554\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 110s 825ms/step - loss: 14.8908 - f1_score: 0.1238 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.54554\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 115s 866ms/step - loss: 0.1925 - f1_score: 0.3942 - val_loss: 0.0991 - val_f1_score: 0.5784\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.57839, saving model to ../data/st_model/2_4.att0\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 110s 824ms/step - loss: 0.1477 - f1_score: 0.4967 - val_loss: 0.0887 - val_f1_score: 0.5986\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.57839 to 0.59858, saving model to ../data/st_model/2_4.att0\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 109s 823ms/step - loss: 0.1316 - f1_score: 0.5420 - val_loss: 0.0805 - val_f1_score: 0.6051\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.59858 to 0.60508, saving model to ../data/st_model/2_4.att0\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 110s 824ms/step - loss: 0.1173 - f1_score: 0.5982 - val_loss: 0.0808 - val_f1_score: 0.6110\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.60508 to 0.61095, saving model to ../data/st_model/2_4.att0\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 110s 824ms/step - loss: 0.1030 - f1_score: 0.6581 - val_loss: 0.0845 - val_f1_score: 0.6092\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.61095\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 109s 823ms/step - loss: 0.0894 - f1_score: 0.7157 - val_loss: 0.0928 - val_f1_score: 0.6042\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.61095\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 109s 823ms/step - loss: 0.0762 - f1_score: 0.7634 - val_loss: 0.0906 - val_f1_score: 0.6043\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.61095\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 116s 870ms/step - loss: 0.2002 - f1_score: 0.3456 - val_loss: 0.0976 - val_f1_score: 0.5640\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.56402, saving model to ../data/st_model/3_4.att0\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 110s 827ms/step - loss: 0.1522 - f1_score: 0.4737 - val_loss: 0.0857 - val_f1_score: 0.6082\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.56402 to 0.60817, saving model to ../data/st_model/3_4.att0\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 110s 829ms/step - loss: 0.1340 - f1_score: 0.5292 - val_loss: 0.0797 - val_f1_score: 0.6340\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.60817 to 0.63401, saving model to ../data/st_model/3_4.att0\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 110s 826ms/step - loss: 0.1213 - f1_score: 0.5826 - val_loss: 0.0758 - val_f1_score: 0.6265\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.63401\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 110s 826ms/step - loss: 0.1059 - f1_score: 0.6460 - val_loss: 0.0800 - val_f1_score: 0.6213\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.63401\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 110s 829ms/step - loss: 0.0938 - f1_score: 0.6935 - val_loss: 0.0772 - val_f1_score: 0.6378\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.63401 to 0.63777, saving model to ../data/st_model/3_4.att0\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 110s 829ms/step - loss: 0.0904 - f1_score: 0.7075 - val_loss: 0.0782 - val_f1_score: 0.6402\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.63777 to 0.64022, saving model to ../data/st_model/3_4.att0\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 110s 828ms/step - loss: 0.0883 - f1_score: 0.7183 - val_loss: 0.0797 - val_f1_score: 0.6381\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.64022\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 110s 828ms/step - loss: 0.0856 - f1_score: 0.7274 - val_loss: 0.0793 - val_f1_score: 0.6360\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.64022\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 110s 828ms/step - loss: 0.0842 - f1_score: 0.7326 - val_loss: 0.0799 - val_f1_score: 0.6348\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.64022\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 117s 877ms/step - loss: 0.1910 - f1_score: 0.3982 - val_loss: 0.0956 - val_f1_score: 0.5539\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.55390, saving model to ../data/st_model/4_4.att0\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.1494 - f1_score: 0.4950 - val_loss: 0.0886 - val_f1_score: 0.5772\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.55390 to 0.57722, saving model to ../data/st_model/4_4.att0\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 111s 833ms/step - loss: 0.1328 - f1_score: 0.5472 - val_loss: 0.0832 - val_f1_score: 0.6058\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.57722 to 0.60582, saving model to ../data/st_model/4_4.att0\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 110s 830ms/step - loss: 0.1171 - f1_score: 0.6041 - val_loss: 0.0780 - val_f1_score: 0.6204\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.60582 to 0.62039, saving model to ../data/st_model/4_4.att0\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 111s 833ms/step - loss: 0.1021 - f1_score: 0.6631 - val_loss: 0.0820 - val_f1_score: 0.6137\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.62039\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.0894 - f1_score: 0.7154 - val_loss: 0.0885 - val_f1_score: 0.6192\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.62039\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.0752 - f1_score: 0.7685 - val_loss: 0.0883 - val_f1_score: 0.6210\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.62039 to 0.62101, saving model to ../data/st_model/4_4.att0\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 111s 831ms/step - loss: 0.0717 - f1_score: 0.7798 - val_loss: 0.0894 - val_f1_score: 0.6194\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.62101\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 110s 831ms/step - loss: 0.0688 - f1_score: 0.7902 - val_loss: 0.0917 - val_f1_score: 0.6194\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.62101\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 109s 821ms/step - loss: 0.0673 - f1_score: 0.7973 - val_loss: 0.0919 - val_f1_score: 0.6212\n",
      "\n",
      "Epoch 00010: val_f1_score improved from 0.62101 to 0.62117, saving model to ../data/st_model/4_4.att0\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 111s 834ms/step - loss: 0.0670 - f1_score: 0.7979 - val_loss: 0.0922 - val_f1_score: 0.6201\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.62117\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 111s 833ms/step - loss: 0.0675 - f1_score: 0.7984 - val_loss: 0.0920 - val_f1_score: 0.6211\n",
      "\n",
      "Epoch 00012: val_f1_score did not improve from 0.62117\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 110s 830ms/step - loss: 0.0665 - f1_score: 0.7998 - val_loss: 0.0921 - val_f1_score: 0.6211\n",
      "\n",
      "Epoch 00013: val_f1_score did not improve from 0.62117\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    get_result_att0(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_att1(x0,x1):\n",
    "    Embedding_layer0 = Embedding(nb_words + 1, EMBEDDING_DIM, \n",
    "                                weights=[word_embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                name = 'lc_embed0',\n",
    "                                trainable = False,\n",
    "                                )\n",
    "    Embedding_layer1 = Embedding(nb_words + 1, 100, \n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                name = 'lc_embed1',\n",
    "                                trainable = True,\n",
    "                                )\n",
    "    Embedding_layer2 = Embedding(nb_words1 + 1, EMBEDDING_DIM, \n",
    "                                weights=[word_embedding_matrix1],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH1, \n",
    "                                name = 'lc_embed2',\n",
    "                                trainable = False,\n",
    "                                )\n",
    "    Embedding_layer3 = Embedding(nb_words1 + 1, 100, \n",
    "                                input_length=MAX_SEQUENCE_LENGTH1, \n",
    "                                name = 'lc_embed3',\n",
    "                                trainable = True,\n",
    "                                )\n",
    "    \n",
    "    x = Embedding_layer0(x0)\n",
    "    x01 = Embedding_layer1(x0)\n",
    "    x2 = Embedding_layer2(x1)\n",
    "    x3 = Embedding_layer3(x1)\n",
    "    x01 = SpatialDropout1D(0.5)(x01)\n",
    "    x3 = SpatialDropout1D(0.5)(x3)\n",
    "    \n",
    "    xa0 = Concatenate(axis=2)([x,x01])\n",
    "    xb0 = Concatenate(axis=2)([x2,x3])\n",
    "        \n",
    "    xa = Bidirectional(LSTM(128, return_sequences=True,dropout=0.5,recurrent_dropout=0.5))(xa0) \n",
    "    xa = Bidirectional(LSTM(64, return_sequences=True,dropout=0.5,recurrent_dropout=0.5))(xa) \n",
    "    \n",
    "    xb = Bidirectional(LSTM(128, return_sequences=True,dropout=0.5,recurrent_dropout=0.5))(xb0)\n",
    "    xb = Bidirectional(LSTM(64, return_sequences=True,dropout=0.5,recurrent_dropout=0.5))(xb) \n",
    "    xa2 = AttentionWithContext()(xa)\n",
    "    xb2 = AttentionWithContext()(xb)\n",
    "\n",
    "    ya = xa2\n",
    "    yb = xb2\n",
    "    \n",
    "    ya = Dropout(0.5)(ya)\n",
    "    yb = Dropout(0.5)(yb)\n",
    "    y = Concatenate(axis=1)([ya,yb])\n",
    "    y = Dense(30, kernel_initializer='he_normal', activation='sigmoid')(y)        \n",
    "    return y\n",
    "    \n",
    "#inputxa = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "#inputxb = Input(shape=(MAX_SEQUENCE_LENGTH1,))\n",
    "#outputya = get_model_att0(inputxa,inputxb)\n",
    "#model1 = Model([inputxa,inputxb], outputya)\n",
    "#model1.summary() \n",
    "\n",
    "def get_result_att1(timei):\n",
    "    xx_cv = []\n",
    "    xx_pre = []\n",
    "    xx_train = []\n",
    "    yy_train = []\n",
    "    early_stopping = EarlyStopping(patience=3,\n",
    "                                   verbose=1,\n",
    "                                   monitor='val_f1_score',\n",
    "                                   mode='max'\n",
    "                                  )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(factor=0.1, \n",
    "                                  patience=2, \n",
    "                                  min_lr=0.00001, \n",
    "                                  verbose=1,\n",
    "                                  monitor='val_f1_score',\n",
    "                                  mode='max',\n",
    "                                 )\n",
    "    foldi = -1\n",
    "    for train_in,test_in in skf.split(train_words,Y):\n",
    "        foldi = foldi+1\n",
    "        X_train = []\n",
    "        X_traina = []\n",
    "        for i in train_in:\n",
    "            X_train.append(train_words[i])\n",
    "            X_traina.append(train.loc[i,'words1'])\n",
    "\n",
    "        X_test = []\n",
    "        for i in test_in:\n",
    "            X_test.append(train_words[i])\n",
    "\n",
    "        X_train1 = []\n",
    "        for i in train_in:\n",
    "            X_train1.append(train_chars[i])\n",
    "\n",
    "        X_test1 = []\n",
    "        for i in test_in:\n",
    "            X_test1.append(train_chars[i])\n",
    "\n",
    "        y_train,y_test = Y[train_in],Y[test_in]\n",
    "\n",
    "        X_test = get_pad_seq(X_test)\n",
    "        X_test1 = get_pad_char_seq(X_test1)\n",
    "\n",
    "        params = {'batch_size': 128,\n",
    "                  'aug':128,\n",
    "                  'shuffle': True}\n",
    "\n",
    "        training_generator = DataGenerator(X_train,X_train1,y_train, **params)\n",
    "\n",
    "        inputxa = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "        inputxb = Input(shape=(MAX_SEQUENCE_LENGTH1,))\n",
    "        outputya = get_model_att1(inputxa,inputxb)\n",
    "        model1 = Model([inputxa,inputxb], outputya)\n",
    "\n",
    "\n",
    "        model1.compile(loss='binary_crossentropy', \n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[f1_score],\n",
    "                     )\n",
    "        filename = st_model+str(foldi)+'_'+str(timei)+'.att1'\n",
    "        model_checkpoint = ModelCheckpoint(filename,\n",
    "                                       save_best_only=True,\n",
    "                                       verbose=1,\n",
    "                                       monitor='val_f1_score',\n",
    "                                       mode='max'\n",
    "                                      )\n",
    "            \n",
    "        history =model1.fit_generator(generator=training_generator,\n",
    "                        validation_data=[[X_test,X_test1],y_test],\n",
    "                        epochs=100,\n",
    "                        callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                       )    \n",
    "\n",
    "        model1.load_weights(filename)\n",
    "        ttesty = model1.predict([test_X,test_X1],batch_size = 128)\n",
    "        xx_pre.append(ttesty)\n",
    "        xx_cv.append(np.max(history.history['val_f1_score']))\n",
    "\n",
    "        ttesty = model1.predict([X_test,X_test1],batch_size = 128)\n",
    "        xx_train.append(ttesty)\n",
    "        yy_train.append(y_test)\n",
    "    \n",
    "    s = 0\n",
    "    for i in xx_pre:\n",
    "        s = s+i\n",
    "    s = s/5\n",
    "    \n",
    "    cols = []\n",
    "    for j in range(30):\n",
    "        cols.append(str(j))\n",
    "    res = pd.DataFrame(s,columns=cols)\n",
    "    res.to_csv(st_csv+str(foldi)+'_'+str(timei)+'_att1.csv',index=None)\n",
    "    \n",
    "    train_res = np.concatenate(xx_train, axis=0)\n",
    "    train_yy = np.concatenate(yy_train, axis=0)\n",
    "    \n",
    "    train_res = pd.DataFrame(train_res,columns=cols)\n",
    "    train_res.to_csv(st_csv+str(foldi)+'_'+str(timei)+'_att1_train.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "66/66 [==============================] - 132s 2s/step - loss: 0.2558 - f1_score: 0.0666 - val_loss: 0.1482 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.2006 - f1_score: 0.0395 - val_loss: 0.1463 - val_f1_score: 0.0111\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.00000 to 0.01106, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1891 - f1_score: 0.1857 - val_loss: 0.1296 - val_f1_score: 0.2731\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.01106 to 0.27308, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1756 - f1_score: 0.3062 - val_loss: 0.1214 - val_f1_score: 0.3692\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.27308 to 0.36915, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1705 - f1_score: 0.3265 - val_loss: 0.1175 - val_f1_score: 0.3909\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.36915 to 0.39085, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1659 - f1_score: 0.3376 - val_loss: 0.1131 - val_f1_score: 0.3924\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.39085 to 0.39239, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1611 - f1_score: 0.3457 - val_loss: 0.1070 - val_f1_score: 0.4633\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.39239 to 0.46331, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1504 - f1_score: 0.3993 - val_loss: 0.0954 - val_f1_score: 0.5031\n",
      "\n",
      "Epoch 00008: val_f1_score improved from 0.46331 to 0.50314, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1377 - f1_score: 0.4583 - val_loss: 0.0870 - val_f1_score: 0.5618\n",
      "\n",
      "Epoch 00009: val_f1_score improved from 0.50314 to 0.56175, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1303 - f1_score: 0.4877 - val_loss: 0.0818 - val_f1_score: 0.5767\n",
      "\n",
      "Epoch 00010: val_f1_score improved from 0.56175 to 0.57675, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1233 - f1_score: 0.5116 - val_loss: 0.0783 - val_f1_score: 0.5806\n",
      "\n",
      "Epoch 00011: val_f1_score improved from 0.57675 to 0.58064, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1202 - f1_score: 0.5221 - val_loss: 0.0758 - val_f1_score: 0.5916\n",
      "\n",
      "Epoch 00012: val_f1_score improved from 0.58064 to 0.59161, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 13/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1175 - f1_score: 0.5250 - val_loss: 0.0760 - val_f1_score: 0.5979\n",
      "\n",
      "Epoch 00013: val_f1_score improved from 0.59161 to 0.59791, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 14/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1138 - f1_score: 0.5418 - val_loss: 0.0738 - val_f1_score: 0.5988\n",
      "\n",
      "Epoch 00014: val_f1_score improved from 0.59791 to 0.59881, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 15/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1109 - f1_score: 0.5541 - val_loss: 0.0753 - val_f1_score: 0.5989\n",
      "\n",
      "Epoch 00015: val_f1_score improved from 0.59881 to 0.59888, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 16/100\n",
      "66/66 [==============================] - 121s 2s/step - loss: 0.1093 - f1_score: 0.5520 - val_loss: 0.0741 - val_f1_score: 0.6103\n",
      "\n",
      "Epoch 00016: val_f1_score improved from 0.59888 to 0.61034, saving model to ../data/st_model/0_4.att1\n",
      "Epoch 17/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1063 - f1_score: 0.5698 - val_loss: 0.0742 - val_f1_score: 0.6049\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.61034\n",
      "Epoch 18/100\n",
      "66/66 [==============================] - 131s 2s/step - loss: 0.1036 - f1_score: 0.5798 - val_loss: 0.0743 - val_f1_score: 0.6048\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.61034\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/100\n",
      "66/66 [==============================] - 133s 2s/step - loss: 0.1011 - f1_score: 0.5872 - val_loss: 0.0743 - val_f1_score: 0.6056\n",
      "\n",
      "Epoch 00019: val_f1_score did not improve from 0.61034\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/100\n",
      "66/66 [==============================] - 143s 2s/step - loss: 0.2545 - f1_score: 0.0503 - val_loss: 0.1514 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 132s 2s/step - loss: 0.2001 - f1_score: 0.0235 - val_loss: 0.1499 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1963 - f1_score: 0.0225 - val_loss: 0.1498 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1943 - f1_score: 0.0282 - val_loss: 0.1484 - val_f1_score: 0.0047\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.00000 to 0.00469, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1924 - f1_score: 0.0553 - val_loss: 0.1460 - val_f1_score: 0.0732\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.00469 to 0.07315, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1896 - f1_score: 0.1174 - val_loss: 0.1428 - val_f1_score: 0.1791\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.07315 to 0.17906, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1876 - f1_score: 0.1732 - val_loss: 0.1399 - val_f1_score: 0.2299\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.17906 to 0.22987, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1853 - f1_score: 0.1969 - val_loss: 0.1371 - val_f1_score: 0.2529\n",
      "\n",
      "Epoch 00008: val_f1_score improved from 0.22987 to 0.25293, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1823 - f1_score: 0.2056 - val_loss: 0.1347 - val_f1_score: 0.2708\n",
      "\n",
      "Epoch 00009: val_f1_score improved from 0.25293 to 0.27081, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1790 - f1_score: 0.2285 - val_loss: 0.1321 - val_f1_score: 0.2841\n",
      "\n",
      "Epoch 00010: val_f1_score improved from 0.27081 to 0.28407, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1765 - f1_score: 0.2485 - val_loss: 0.1301 - val_f1_score: 0.3452\n",
      "\n",
      "Epoch 00011: val_f1_score improved from 0.28407 to 0.34520, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1750 - f1_score: 0.2738 - val_loss: 0.1278 - val_f1_score: 0.3555\n",
      "\n",
      "Epoch 00012: val_f1_score improved from 0.34520 to 0.35554, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 13/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1723 - f1_score: 0.3019 - val_loss: 0.1258 - val_f1_score: 0.3586\n",
      "\n",
      "Epoch 00013: val_f1_score improved from 0.35554 to 0.35860, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 14/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1702 - f1_score: 0.3087 - val_loss: 0.1246 - val_f1_score: 0.3619\n",
      "\n",
      "Epoch 00014: val_f1_score improved from 0.35860 to 0.36193, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 15/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1679 - f1_score: 0.3223 - val_loss: 0.1223 - val_f1_score: 0.3607\n",
      "\n",
      "Epoch 00015: val_f1_score did not improve from 0.36193\n",
      "Epoch 16/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1670 - f1_score: 0.3241 - val_loss: 0.1207 - val_f1_score: 0.3631\n",
      "\n",
      "Epoch 00016: val_f1_score improved from 0.36193 to 0.36313, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 17/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1651 - f1_score: 0.3320 - val_loss: 0.1192 - val_f1_score: 0.3623\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.36313\n",
      "Epoch 18/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1633 - f1_score: 0.3391 - val_loss: 0.1181 - val_f1_score: 0.4093\n",
      "\n",
      "Epoch 00018: val_f1_score improved from 0.36313 to 0.40933, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 19/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1619 - f1_score: 0.3437 - val_loss: 0.1167 - val_f1_score: 0.4375\n",
      "\n",
      "Epoch 00019: val_f1_score improved from 0.40933 to 0.43746, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 20/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1611 - f1_score: 0.3417 - val_loss: 0.1157 - val_f1_score: 0.4472\n",
      "\n",
      "Epoch 00020: val_f1_score improved from 0.43746 to 0.44723, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 21/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1575 - f1_score: 0.3571 - val_loss: 0.1144 - val_f1_score: 0.4512\n",
      "\n",
      "Epoch 00021: val_f1_score improved from 0.44723 to 0.45123, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 22/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1581 - f1_score: 0.3604 - val_loss: 0.1132 - val_f1_score: 0.4554\n",
      "\n",
      "Epoch 00022: val_f1_score improved from 0.45123 to 0.45536, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 23/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1566 - f1_score: 0.3670 - val_loss: 0.1122 - val_f1_score: 0.4603\n",
      "\n",
      "Epoch 00023: val_f1_score improved from 0.45536 to 0.46026, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 24/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1555 - f1_score: 0.3707 - val_loss: 0.1108 - val_f1_score: 0.4611\n",
      "\n",
      "Epoch 00024: val_f1_score improved from 0.46026 to 0.46108, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 25/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1533 - f1_score: 0.3756 - val_loss: 0.1095 - val_f1_score: 0.4673\n",
      "\n",
      "Epoch 00025: val_f1_score improved from 0.46108 to 0.46731, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 26/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1521 - f1_score: 0.3846 - val_loss: 0.1083 - val_f1_score: 0.4740\n",
      "\n",
      "Epoch 00026: val_f1_score improved from 0.46731 to 0.47402, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 27/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1508 - f1_score: 0.3875 - val_loss: 0.1074 - val_f1_score: 0.4757\n",
      "\n",
      "Epoch 00027: val_f1_score improved from 0.47402 to 0.47566, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 28/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1494 - f1_score: 0.3904 - val_loss: 0.1066 - val_f1_score: 0.4818\n",
      "\n",
      "Epoch 00028: val_f1_score improved from 0.47566 to 0.48181, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 29/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1481 - f1_score: 0.3971 - val_loss: 0.1051 - val_f1_score: 0.4860\n",
      "\n",
      "Epoch 00029: val_f1_score improved from 0.48181 to 0.48599, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 30/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1470 - f1_score: 0.4013 - val_loss: 0.1039 - val_f1_score: 0.4885\n",
      "\n",
      "Epoch 00030: val_f1_score improved from 0.48599 to 0.48855, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 31/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1463 - f1_score: 0.4034 - val_loss: 0.1026 - val_f1_score: 0.4899\n",
      "\n",
      "Epoch 00031: val_f1_score improved from 0.48855 to 0.48989, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 32/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1437 - f1_score: 0.4092 - val_loss: 0.1017 - val_f1_score: 0.4935\n",
      "\n",
      "Epoch 00032: val_f1_score improved from 0.48989 to 0.49347, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 33/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1428 - f1_score: 0.4200 - val_loss: 0.1007 - val_f1_score: 0.4927\n",
      "\n",
      "Epoch 00033: val_f1_score did not improve from 0.49347\n",
      "Epoch 34/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1429 - f1_score: 0.4204 - val_loss: 0.1001 - val_f1_score: 0.4954\n",
      "\n",
      "Epoch 00034: val_f1_score improved from 0.49347 to 0.49536, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 35/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1417 - f1_score: 0.4232 - val_loss: 0.0995 - val_f1_score: 0.5047\n",
      "\n",
      "Epoch 00035: val_f1_score improved from 0.49536 to 0.50469, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 36/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1401 - f1_score: 0.4310 - val_loss: 0.0986 - val_f1_score: 0.5109\n",
      "\n",
      "Epoch 00036: val_f1_score improved from 0.50469 to 0.51087, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 37/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1397 - f1_score: 0.4321 - val_loss: 0.0972 - val_f1_score: 0.5127\n",
      "\n",
      "Epoch 00037: val_f1_score improved from 0.51087 to 0.51275, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 38/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1383 - f1_score: 0.4368 - val_loss: 0.0968 - val_f1_score: 0.5152\n",
      "\n",
      "Epoch 00038: val_f1_score improved from 0.51275 to 0.51520, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 39/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1369 - f1_score: 0.4453 - val_loss: 0.0961 - val_f1_score: 0.5173\n",
      "\n",
      "Epoch 00039: val_f1_score improved from 0.51520 to 0.51725, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 40/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1364 - f1_score: 0.4522 - val_loss: 0.0957 - val_f1_score: 0.5194\n",
      "\n",
      "Epoch 00040: val_f1_score improved from 0.51725 to 0.51942, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 41/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1341 - f1_score: 0.4550 - val_loss: 0.0950 - val_f1_score: 0.5193\n",
      "\n",
      "Epoch 00041: val_f1_score did not improve from 0.51942\n",
      "Epoch 42/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1347 - f1_score: 0.4546 - val_loss: 0.0944 - val_f1_score: 0.5216\n",
      "\n",
      "Epoch 00042: val_f1_score improved from 0.51942 to 0.52159, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 43/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1330 - f1_score: 0.4612 - val_loss: 0.0939 - val_f1_score: 0.5234\n",
      "\n",
      "Epoch 00043: val_f1_score improved from 0.52159 to 0.52342, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 44/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1328 - f1_score: 0.4617 - val_loss: 0.0936 - val_f1_score: 0.5269\n",
      "\n",
      "Epoch 00044: val_f1_score improved from 0.52342 to 0.52690, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 45/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1310 - f1_score: 0.4685 - val_loss: 0.0925 - val_f1_score: 0.5267\n",
      "\n",
      "Epoch 00045: val_f1_score did not improve from 0.52690\n",
      "Epoch 46/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1309 - f1_score: 0.4713 - val_loss: 0.0924 - val_f1_score: 0.5331\n",
      "\n",
      "Epoch 00046: val_f1_score improved from 0.52690 to 0.53311, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 47/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1306 - f1_score: 0.4766 - val_loss: 0.0916 - val_f1_score: 0.5363\n",
      "\n",
      "Epoch 00047: val_f1_score improved from 0.53311 to 0.53626, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 48/100\n",
      "66/66 [==============================] - 122s 2s/step - loss: 0.1296 - f1_score: 0.4768 - val_loss: 0.0916 - val_f1_score: 0.5438\n",
      "\n",
      "Epoch 00048: val_f1_score improved from 0.53626 to 0.54385, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 49/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1297 - f1_score: 0.4704 - val_loss: 0.0910 - val_f1_score: 0.5452\n",
      "\n",
      "Epoch 00049: val_f1_score improved from 0.54385 to 0.54520, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 50/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1289 - f1_score: 0.4791 - val_loss: 0.0907 - val_f1_score: 0.5485\n",
      "\n",
      "Epoch 00050: val_f1_score improved from 0.54520 to 0.54848, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 51/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1279 - f1_score: 0.4836 - val_loss: 0.0903 - val_f1_score: 0.5476\n",
      "\n",
      "Epoch 00051: val_f1_score did not improve from 0.54848\n",
      "Epoch 52/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1270 - f1_score: 0.4877 - val_loss: 0.0902 - val_f1_score: 0.5529\n",
      "\n",
      "Epoch 00052: val_f1_score improved from 0.54848 to 0.55291, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 53/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1265 - f1_score: 0.4896 - val_loss: 0.0898 - val_f1_score: 0.5571\n",
      "\n",
      "Epoch 00053: val_f1_score improved from 0.55291 to 0.55712, saving model to ../data/st_model/1_4.att1\n",
      "Epoch 54/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1263 - f1_score: 0.4922 - val_loss: 0.0894 - val_f1_score: 0.5548\n",
      "\n",
      "Epoch 00054: val_f1_score did not improve from 0.55712\n",
      "Epoch 55/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1255 - f1_score: 0.4948 - val_loss: 0.0892 - val_f1_score: 0.5561\n",
      "\n",
      "Epoch 00055: val_f1_score did not improve from 0.55712\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 56/100\n",
      "66/66 [==============================] - 123s 2s/step - loss: 0.1261 - f1_score: 0.4957 - val_loss: 0.0890 - val_f1_score: 0.5558\n",
      "\n",
      "Epoch 00056: val_f1_score did not improve from 0.55712\n",
      "Epoch 00056: early stopping\n",
      "Epoch 1/100\n",
      "66/66 [==============================] - 133s 2s/step - loss: 0.2511 - f1_score: 0.0617 - val_loss: 0.1510 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.2005 - f1_score: 0.0491 - val_loss: 0.1495 - val_f1_score: 0.0054\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.00000 to 0.00543, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1894 - f1_score: 0.1761 - val_loss: 0.1362 - val_f1_score: 0.2610\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.00543 to 0.26101, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1791 - f1_score: 0.2755 - val_loss: 0.1259 - val_f1_score: 0.3609\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.26101 to 0.36089, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1703 - f1_score: 0.3265 - val_loss: 0.1215 - val_f1_score: 0.3580\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.36089\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1655 - f1_score: 0.3217 - val_loss: 0.1173 - val_f1_score: 0.3753\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.36089 to 0.37527, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1576 - f1_score: 0.3454 - val_loss: 0.1060 - val_f1_score: 0.4806\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.37527 to 0.48056, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1470 - f1_score: 0.3850 - val_loss: 0.0955 - val_f1_score: 0.5006\n",
      "\n",
      "Epoch 00008: val_f1_score improved from 0.48056 to 0.50056, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1359 - f1_score: 0.4319 - val_loss: 0.0880 - val_f1_score: 0.5675\n",
      "\n",
      "Epoch 00009: val_f1_score improved from 0.50056 to 0.56745, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1266 - f1_score: 0.4789 - val_loss: 0.0826 - val_f1_score: 0.5704\n",
      "\n",
      "Epoch 00010: val_f1_score improved from 0.56745 to 0.57042, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1223 - f1_score: 0.5053 - val_loss: 0.0817 - val_f1_score: 0.5873\n",
      "\n",
      "Epoch 00011: val_f1_score improved from 0.57042 to 0.58731, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1187 - f1_score: 0.5190 - val_loss: 0.0796 - val_f1_score: 0.5929\n",
      "\n",
      "Epoch 00012: val_f1_score improved from 0.58731 to 0.59294, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 13/100\n",
      "66/66 [==============================] - 124s 2s/step - loss: 0.1149 - f1_score: 0.5352 - val_loss: 0.0776 - val_f1_score: 0.5927\n",
      "\n",
      "Epoch 00013: val_f1_score did not improve from 0.59294\n",
      "Epoch 14/100\n",
      "66/66 [==============================] - 126s 2s/step - loss: 0.1106 - f1_score: 0.5514 - val_loss: 0.0790 - val_f1_score: 0.5983\n",
      "\n",
      "Epoch 00014: val_f1_score improved from 0.59294 to 0.59827, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 15/100\n",
      "66/66 [==============================] - 126s 2s/step - loss: 0.1094 - f1_score: 0.5607 - val_loss: 0.0786 - val_f1_score: 0.6007\n",
      "\n",
      "Epoch 00015: val_f1_score improved from 0.59827 to 0.60067, saving model to ../data/st_model/2_4.att1\n",
      "Epoch 16/100\n",
      "66/66 [==============================] - 125s 2s/step - loss: 0.1071 - f1_score: 0.5662 - val_loss: 0.0785 - val_f1_score: 0.5784\n",
      "\n",
      "Epoch 00016: val_f1_score did not improve from 0.60067\n",
      "Epoch 17/100\n",
      "66/66 [==============================] - 127s 2s/step - loss: 0.1049 - f1_score: 0.5745 - val_loss: 0.0781 - val_f1_score: 0.5869\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.60067\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 18/100\n",
      "66/66 [==============================] - 127s 2s/step - loss: 0.1027 - f1_score: 0.5837 - val_loss: 0.0780 - val_f1_score: 0.5855\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.60067\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "66/66 [==============================] - 139s 2s/step - loss: 0.2565 - f1_score: 0.0619 - val_loss: 0.1477 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 129s 2s/step - loss: 0.2012 - f1_score: 0.0369 - val_loss: 0.1461 - val_f1_score: 0.0080\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.00000 to 0.00800, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 129s 2s/step - loss: 0.1962 - f1_score: 0.0611 - val_loss: 0.1353 - val_f1_score: 0.2022\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.00800 to 0.20224, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 127s 2s/step - loss: 0.1806 - f1_score: 0.2637 - val_loss: 0.1230 - val_f1_score: 0.3778\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.20224 to 0.37784, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 127s 2s/step - loss: 0.1719 - f1_score: 0.3270 - val_loss: 0.1176 - val_f1_score: 0.3818\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.37784 to 0.38176, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 126s 2s/step - loss: 0.1680 - f1_score: 0.3198 - val_loss: 0.1121 - val_f1_score: 0.3896\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.38176 to 0.38960, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 126s 2s/step - loss: 0.1588 - f1_score: 0.3478 - val_loss: 0.1035 - val_f1_score: 0.4762\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.38960 to 0.47624, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 126s 2s/step - loss: 0.1482 - f1_score: 0.3830 - val_loss: 0.0920 - val_f1_score: 0.5304\n",
      "\n",
      "Epoch 00008: val_f1_score improved from 0.47624 to 0.53038, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 127s 2s/step - loss: 0.1361 - f1_score: 0.4465 - val_loss: 0.0849 - val_f1_score: 0.5793\n",
      "\n",
      "Epoch 00009: val_f1_score improved from 0.53038 to 0.57928, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 128s 2s/step - loss: 0.1275 - f1_score: 0.4833 - val_loss: 0.0799 - val_f1_score: 0.5957\n",
      "\n",
      "Epoch 00010: val_f1_score improved from 0.57928 to 0.59566, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 128s 2s/step - loss: 0.1230 - f1_score: 0.5068 - val_loss: 0.0776 - val_f1_score: 0.6093\n",
      "\n",
      "Epoch 00011: val_f1_score improved from 0.59566 to 0.60934, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 128s 2s/step - loss: 0.1189 - f1_score: 0.5177 - val_loss: 0.0759 - val_f1_score: 0.6120\n",
      "\n",
      "Epoch 00012: val_f1_score improved from 0.60934 to 0.61200, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 13/100\n",
      "66/66 [==============================] - 129s 2s/step - loss: 0.1166 - f1_score: 0.5313 - val_loss: 0.0757 - val_f1_score: 0.6184\n",
      "\n",
      "Epoch 00013: val_f1_score improved from 0.61200 to 0.61844, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 14/100\n",
      "66/66 [==============================] - 128s 2s/step - loss: 0.1134 - f1_score: 0.5453 - val_loss: 0.0746 - val_f1_score: 0.6178\n",
      "\n",
      "Epoch 00014: val_f1_score did not improve from 0.61844\n",
      "Epoch 15/100\n",
      "66/66 [==============================] - 126s 2s/step - loss: 0.1123 - f1_score: 0.5434 - val_loss: 0.0740 - val_f1_score: 0.6204\n",
      "\n",
      "Epoch 00015: val_f1_score improved from 0.61844 to 0.62045, saving model to ../data/st_model/3_4.att1\n",
      "Epoch 16/100\n",
      "66/66 [==============================] - 126s 2s/step - loss: 0.1098 - f1_score: 0.5516 - val_loss: 0.0741 - val_f1_score: 0.6198\n",
      "\n",
      "Epoch 00016: val_f1_score did not improve from 0.62045\n",
      "Epoch 17/100\n",
      "66/66 [==============================] - 126s 2s/step - loss: 0.1073 - f1_score: 0.5628 - val_loss: 0.0743 - val_f1_score: 0.6205\n",
      "\n",
      "Epoch 00017: val_f1_score improved from 0.62045 to 0.62050, saving model to ../data/st_model/3_4.att1\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 18/100\n",
      "66/66 [==============================] - 126s 2s/step - loss: 0.1054 - f1_score: 0.5751 - val_loss: 0.0733 - val_f1_score: 0.6196\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.62050\n",
      "Epoch 19/100\n",
      "66/66 [==============================] - 127s 2s/step - loss: 0.1049 - f1_score: 0.5704 - val_loss: 0.0735 - val_f1_score: 0.6199\n",
      "\n",
      "Epoch 00019: val_f1_score did not improve from 0.62050\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 20/100\n",
      "66/66 [==============================] - 127s 2s/step - loss: 0.1050 - f1_score: 0.5709 - val_loss: 0.0734 - val_f1_score: 0.6203\n",
      "\n",
      "Epoch 00020: val_f1_score did not improve from 0.62050\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/100\n",
      "66/66 [==============================] - 139s 2s/step - loss: 0.2529 - f1_score: 0.0570 - val_loss: 0.1501 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 128s 2s/step - loss: 0.2002 - f1_score: 0.0274 - val_loss: 0.1481 - val_f1_score: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 128s 2s/step - loss: 0.1957 - f1_score: 0.0626 - val_loss: 0.1397 - val_f1_score: 0.2242\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.00000 to 0.22422, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 128s 2s/step - loss: 0.1831 - f1_score: 0.2397 - val_loss: 0.1270 - val_f1_score: 0.3471\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.22422 to 0.34714, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 128s 2s/step - loss: 0.1727 - f1_score: 0.3175 - val_loss: 0.1244 - val_f1_score: 0.3597\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.34714 to 0.35970, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 129s 2s/step - loss: 0.1651 - f1_score: 0.3474 - val_loss: 0.1113 - val_f1_score: 0.4541\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.35970 to 0.45411, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 128s 2s/step - loss: 0.1499 - f1_score: 0.4216 - val_loss: 0.1015 - val_f1_score: 0.4865\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.45411 to 0.48649, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 128s 2s/step - loss: 0.1429 - f1_score: 0.4462 - val_loss: 0.0947 - val_f1_score: 0.5133\n",
      "\n",
      "Epoch 00008: val_f1_score improved from 0.48649 to 0.51327, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 128s 2s/step - loss: 0.1360 - f1_score: 0.4657 - val_loss: 0.0906 - val_f1_score: 0.5204\n",
      "\n",
      "Epoch 00009: val_f1_score improved from 0.51327 to 0.52035, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 129s 2s/step - loss: 0.1304 - f1_score: 0.4806 - val_loss: 0.0870 - val_f1_score: 0.5478\n",
      "\n",
      "Epoch 00010: val_f1_score improved from 0.52035 to 0.54776, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 130s 2s/step - loss: 0.1258 - f1_score: 0.4986 - val_loss: 0.0824 - val_f1_score: 0.5674\n",
      "\n",
      "Epoch 00011: val_f1_score improved from 0.54776 to 0.56744, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 131s 2s/step - loss: 0.1216 - f1_score: 0.5122 - val_loss: 0.0807 - val_f1_score: 0.5839\n",
      "\n",
      "Epoch 00012: val_f1_score improved from 0.56744 to 0.58388, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 13/100\n",
      "66/66 [==============================] - 129s 2s/step - loss: 0.1179 - f1_score: 0.5297 - val_loss: 0.0805 - val_f1_score: 0.5863\n",
      "\n",
      "Epoch 00013: val_f1_score improved from 0.58388 to 0.58629, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 14/100\n",
      "66/66 [==============================] - 130s 2s/step - loss: 0.1148 - f1_score: 0.5325 - val_loss: 0.0791 - val_f1_score: 0.5885\n",
      "\n",
      "Epoch 00014: val_f1_score improved from 0.58629 to 0.58849, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 15/100\n",
      "66/66 [==============================] - 131s 2s/step - loss: 0.1119 - f1_score: 0.5482 - val_loss: 0.0770 - val_f1_score: 0.5868\n",
      "\n",
      "Epoch 00015: val_f1_score did not improve from 0.58849\n",
      "Epoch 16/100\n",
      "66/66 [==============================] - 131s 2s/step - loss: 0.1095 - f1_score: 0.5541 - val_loss: 0.0776 - val_f1_score: 0.5921\n",
      "\n",
      "Epoch 00016: val_f1_score improved from 0.58849 to 0.59211, saving model to ../data/st_model/4_4.att1\n",
      "Epoch 17/100\n",
      "66/66 [==============================] - 130s 2s/step - loss: 0.1068 - f1_score: 0.5625 - val_loss: 0.0769 - val_f1_score: 0.5872\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.59211\n",
      "Epoch 18/100\n",
      "66/66 [==============================] - 130s 2s/step - loss: 0.1034 - f1_score: 0.5738 - val_loss: 0.0771 - val_f1_score: 0.5843\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.59211\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/100\n",
      "66/66 [==============================] - 130s 2s/step - loss: 0.1017 - f1_score: 0.5791 - val_loss: 0.0770 - val_f1_score: 0.5905\n",
      "\n",
      "Epoch 00019: val_f1_score did not improve from 0.59211\n",
      "Epoch 00019: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    get_result_att1(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lc_embed1 (Embedding)           (None, 128, 100)     990600      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lc_embed3 (Embedding)           (None, 200, 100)     281300      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lc_embed0 (Embedding)           (None, 128, 300)     2971800     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_5 (SpatialDro (None, 128, 100)     0           lc_embed1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lc_embed2 (Embedding)           (None, 200, 300)     843900      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_6 (SpatialDro (None, 200, 100)     0           lc_embed3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128, 400)     0           lc_embed0[0][0]                  \n",
      "                                                                 spatial_dropout1d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 200, 400)     0           lc_embed2[0][0]                  \n",
      "                                                                 spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 128, 512)     1009152     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 200, 512)     1009152     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_5 (Atten (None, 512)          263168      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_6 (Atten (None, 512)          263168      bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 512)          0           attention_with_context_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 512)          0           attention_with_context_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 30)           15390       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 30)           15390       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 30)           0           dense_5[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,663,020\n",
      "Trainable params: 3,847,320\n",
      "Non-trainable params: 3,815,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model_att2(x0,x1):\n",
    "    Embedding_layer0 = Embedding(nb_words + 1, EMBEDDING_DIM, \n",
    "                                weights=[word_embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                name = 'lc_embed0',\n",
    "                                trainable = False,\n",
    "                                )\n",
    "    Embedding_layer1 = Embedding(nb_words + 1, 100, \n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                name = 'lc_embed1',\n",
    "                                trainable = True,\n",
    "                                )\n",
    "    Embedding_layer2 = Embedding(nb_words1 + 1, EMBEDDING_DIM, \n",
    "                                weights=[word_embedding_matrix1],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH1, \n",
    "                                name = 'lc_embed2',\n",
    "                                trainable = False,\n",
    "                                )\n",
    "    Embedding_layer3 = Embedding(nb_words1 + 1, 100, \n",
    "                                input_length=MAX_SEQUENCE_LENGTH1, \n",
    "                                name = 'lc_embed3',\n",
    "                                trainable = True,\n",
    "                                )\n",
    "    \n",
    "    x = Embedding_layer0(x0)\n",
    "    x01 = Embedding_layer1(x0)\n",
    "    x2 = Embedding_layer2(x1)\n",
    "    x3 = Embedding_layer3(x1)\n",
    "    x01 = SpatialDropout1D(0.5)(x01)\n",
    "    x3 = SpatialDropout1D(0.5)(x3)\n",
    "    \n",
    "    xa0 = Concatenate(axis=2)([x,x01])\n",
    "    xb0 = Concatenate(axis=2)([x2,x3])\n",
    "        \n",
    "    xa = Bidirectional(GRU(256, return_sequences=True,dropout=0.25))(xa0)    \n",
    "    xb = Bidirectional(GRU(256, return_sequences=True,dropout=0.25))(xb0)\n",
    "\n",
    "    xa2 = AttentionWithContext()(xa)\n",
    "    xb2 = AttentionWithContext()(xb)\n",
    "\n",
    "    ya = xa2\n",
    "    yb = xb2\n",
    "    \n",
    "    ya = Dropout(0.5)(ya)\n",
    "    yb = Dropout(0.5)(yb)\n",
    "    \n",
    "    ya = Dense(30, kernel_initializer='he_normal', activation='sigmoid')(ya)   \n",
    "    yb = Dense(30, kernel_initializer='he_normal', activation='sigmoid')(yb)   \n",
    "    y = Lambda(lambda x:(x[0]+x[1])/2)([ya,yb])\n",
    "    return y\n",
    "    \n",
    "inputxa = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "inputxb = Input(shape=(MAX_SEQUENCE_LENGTH1,))\n",
    "outputya = get_model_att2(inputxa,inputxb)\n",
    "model1 = Model([inputxa,inputxb], outputya)\n",
    "model1.summary() \n",
    "\n",
    "def get_result_att2(timei):\n",
    "    xx_cv = []\n",
    "    xx_pre = []\n",
    "    xx_train = []\n",
    "    yy_train = []\n",
    "    early_stopping = EarlyStopping(patience=3,\n",
    "                                   verbose=1,\n",
    "                                   monitor='val_f1_score',\n",
    "                                   mode='max'\n",
    "                                  )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(factor=0.1, \n",
    "                                  patience=2, \n",
    "                                  min_lr=0.00001, \n",
    "                                  verbose=1,\n",
    "                                  monitor='val_f1_score',\n",
    "                                  mode='max',\n",
    "                                 )\n",
    "    foldi = -1\n",
    "    for train_in,test_in in skf.split(train_words,Y):\n",
    "        foldi = foldi+1\n",
    "        X_train = []\n",
    "        X_traina = []\n",
    "        for i in train_in:\n",
    "            X_train.append(train_words[i])\n",
    "            X_traina.append(train.loc[i,'words1'])\n",
    "\n",
    "        X_test = []\n",
    "        for i in test_in:\n",
    "            X_test.append(train_words[i])\n",
    "\n",
    "        X_train1 = []\n",
    "        for i in train_in:\n",
    "            X_train1.append(train_chars[i])\n",
    "\n",
    "        X_test1 = []\n",
    "        for i in test_in:\n",
    "            X_test1.append(train_chars[i])\n",
    "\n",
    "        y_train,y_test = Y[train_in],Y[test_in]\n",
    "\n",
    "        X_test = get_pad_seq(X_test)\n",
    "        X_test1 = get_pad_char_seq(X_test1)\n",
    "\n",
    "        params = {'batch_size': 64,\n",
    "                  'aug':128+64,\n",
    "                  'shuffle': True}\n",
    "\n",
    "        training_generator = DataGenerator(X_train,X_train1,y_train, **params)\n",
    "\n",
    "        inputxa = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "        inputxb = Input(shape=(MAX_SEQUENCE_LENGTH1,))\n",
    "        outputya = get_model_att2(inputxa,inputxb)\n",
    "        model1 = Model([inputxa,inputxb], outputya)\n",
    "\n",
    "\n",
    "        model1.compile(loss='binary_crossentropy', \n",
    "                      optimizer=\"nadam\",\n",
    "                      metrics=[f1_score],\n",
    "                     )\n",
    "        filename = st_model+str(foldi)+'_'+str(timei)+'.att2'\n",
    "        model_checkpoint = ModelCheckpoint(filename,\n",
    "                                       save_best_only=True,\n",
    "                                       verbose=1,\n",
    "                                       monitor='val_f1_score',\n",
    "                                       mode='max'\n",
    "                                      )\n",
    "            \n",
    "        history =model1.fit_generator(generator=training_generator,\n",
    "                        validation_data=[[X_test,X_test1],y_test],\n",
    "                        epochs=100,\n",
    "                        callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                       )    \n",
    "\n",
    "        model1.load_weights(filename)\n",
    "        ttesty = model1.predict([test_X,test_X1],batch_size = 128)\n",
    "        xx_pre.append(ttesty)\n",
    "        xx_cv.append(np.max(history.history['val_f1_score']))\n",
    "\n",
    "        ttesty = model1.predict([X_test,X_test1],batch_size = 128)\n",
    "        xx_train.append(ttesty)\n",
    "        yy_train.append(y_test)\n",
    "    \n",
    "    s = 0\n",
    "    for i in xx_pre:\n",
    "        s = s+i\n",
    "    s = s/5\n",
    "    \n",
    "    cols = []\n",
    "    for j in range(30):\n",
    "        cols.append(str(j))\n",
    "    res = pd.DataFrame(s,columns=cols)\n",
    "    res.to_csv(st_csv+str(foldi)+'_'+str(timei)+'_att2.csv',index=None)\n",
    "    \n",
    "    train_res = np.concatenate(xx_train, axis=0)\n",
    "    train_yy = np.concatenate(yy_train, axis=0)\n",
    "    \n",
    "    train_res = pd.DataFrame(train_res,columns=cols)\n",
    "    train_res.to_csv(st_csv+str(foldi)+'_'+str(timei)+'_att2_train.csv',index=None)\n",
    "    \n",
    "    #train_yy = pd.DataFrame(train_yy,columns=cols)\n",
    "    #train_yy.to_csv(st_csv+str(foldi)+'_'+str(timei)+'_att2_train_yy.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "133/133 [==============================] - 114s 855ms/step - loss: 11.6909 - f1_score: 0.1433 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07486, saving model to ../data/st_model/0_0.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 110s 828ms/step - loss: 14.8899 - f1_score: 0.1239 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07486\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 110s 829ms/step - loss: 14.8878 - f1_score: 0.1241 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07486\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 113s 853ms/step - loss: 14.8868 - f1_score: 0.1242 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07486\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 115s 863ms/step - loss: 0.1780 - f1_score: 0.3434 - val_loss: 0.0924 - val_f1_score: 0.5396\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.53959, saving model to ../data/st_model/1_0.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 109s 823ms/step - loss: 11.5687 - f1_score: 0.1965 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.53959\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 109s 822ms/step - loss: 14.8952 - f1_score: 0.1233 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.53959\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 109s 821ms/step - loss: 14.8981 - f1_score: 0.1229 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.53959\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 117s 879ms/step - loss: 0.1766 - f1_score: 0.3426 - val_loss: 0.0834 - val_f1_score: 0.5814\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.58139, saving model to ../data/st_model/2_0.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 112s 840ms/step - loss: 13.7871 - f1_score: 0.1469 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.58139\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 112s 843ms/step - loss: 14.8957 - f1_score: 0.1232 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.58139\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 112s 842ms/step - loss: 14.8922 - f1_score: 0.1236 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.58139\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 116s 873ms/step - loss: 12.2446 - f1_score: 0.1359 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07497, saving model to ../data/st_model/3_0.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 111s 834ms/step - loss: 14.8897 - f1_score: 0.1239 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07497\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 111s 833ms/step - loss: 14.8878 - f1_score: 0.1241 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07497\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 111s 835ms/step - loss: 14.8931 - f1_score: 0.1235 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07497\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 115s 868ms/step - loss: 0.1737 - f1_score: 0.3807 - val_loss: 0.0856 - val_f1_score: 0.5699\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.56995, saving model to ../data/st_model/4_0.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.1290 - f1_score: 0.5093 - val_loss: 0.0754 - val_f1_score: 0.5856\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.56995 to 0.58563, saving model to ../data/st_model/4_0.att2\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.1134 - f1_score: 0.5421 - val_loss: 0.0742 - val_f1_score: 0.5967\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.58563 to 0.59670, saving model to ../data/st_model/4_0.att2\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.1002 - f1_score: 0.5639 - val_loss: 0.0791 - val_f1_score: 0.5991\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.59670 to 0.59913, saving model to ../data/st_model/4_0.att2\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 111s 831ms/step - loss: 0.0879 - f1_score: 0.5845 - val_loss: 0.0824 - val_f1_score: 0.6027\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.59913 to 0.60275, saving model to ../data/st_model/4_0.att2\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 0.0777 - f1_score: 0.6073 - val_loss: 0.0886 - val_f1_score: 0.6013\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.60275\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 111s 834ms/step - loss: 0.0687 - f1_score: 0.6390 - val_loss: 0.0971 - val_f1_score: 0.5943\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.60275\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 111s 837ms/step - loss: 0.0612 - f1_score: 0.6559 - val_loss: 0.0998 - val_f1_score: 0.5967\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.60275\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 116s 873ms/step - loss: 0.1806 - f1_score: 0.3497 - val_loss: 0.0809 - val_f1_score: 0.5678\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.56777, saving model to ../data/st_model/0_1.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 111s 835ms/step - loss: 0.1293 - f1_score: 0.4932 - val_loss: 0.0781 - val_f1_score: 0.6075\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.56777 to 0.60753, saving model to ../data/st_model/0_1.att2\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 111s 836ms/step - loss: 0.1129 - f1_score: 0.5284 - val_loss: 0.0723 - val_f1_score: 0.6031\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.60753\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 111s 833ms/step - loss: 11.3397 - f1_score: 0.2230 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.60753\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 111s 832ms/step - loss: 14.8892 - f1_score: 0.1239 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.60753\n",
      "Epoch 00005: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 116s 873ms/step - loss: 12.8053 - f1_score: 0.1295 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07697, saving model to ../data/st_model/1_1.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 110s 826ms/step - loss: 14.8973 - f1_score: 0.1230 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07697\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 110s 830ms/step - loss: 14.8967 - f1_score: 0.1231 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07697\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 110s 830ms/step - loss: 14.8949 - f1_score: 0.1233 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07697\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 118s 885ms/step - loss: 0.1775 - f1_score: 0.3849 - val_loss: 0.0869 - val_f1_score: 0.5763\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.57629, saving model to ../data/st_model/2_1.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 112s 842ms/step - loss: 0.1307 - f1_score: 0.5111 - val_loss: 0.0785 - val_f1_score: 0.6047\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.57629 to 0.60469, saving model to ../data/st_model/2_1.att2\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 112s 841ms/step - loss: 0.1135 - f1_score: 0.5491 - val_loss: 0.0763 - val_f1_score: 0.6086\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.60469 to 0.60862, saving model to ../data/st_model/2_1.att2\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 112s 839ms/step - loss: 0.0996 - f1_score: 0.5800 - val_loss: 0.0795 - val_f1_score: 0.6037\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.60862\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 112s 841ms/step - loss: 0.0863 - f1_score: 0.6110 - val_loss: 0.0848 - val_f1_score: 0.6086\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.60862 to 0.60864, saving model to ../data/st_model/2_1.att2\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 112s 844ms/step - loss: 0.0746 - f1_score: 0.6388 - val_loss: 0.0851 - val_f1_score: 0.5997\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.60864\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 112s 841ms/step - loss: 0.0709 - f1_score: 0.6464 - val_loss: 0.0875 - val_f1_score: 0.6006\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.60864\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 112s 842ms/step - loss: 0.0687 - f1_score: 0.6522 - val_loss: 0.0877 - val_f1_score: 0.6003\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.60864\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 119s 892ms/step - loss: 10.0535 - f1_score: 0.1774 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07497, saving model to ../data/st_model/3_1.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 112s 842ms/step - loss: 14.8865 - f1_score: 0.1242 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07497\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 112s 841ms/step - loss: 14.8899 - f1_score: 0.1239 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07497\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 112s 843ms/step - loss: 14.8873 - f1_score: 0.1241 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07497\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 120s 903ms/step - loss: 9.9402 - f1_score: 0.1720 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07518, saving model to ../data/st_model/4_1.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 113s 847ms/step - loss: 14.8892 - f1_score: 0.1239 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07518\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 113s 850ms/step - loss: 14.8891 - f1_score: 0.1239 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07518\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 113s 853ms/step - loss: 14.8910 - f1_score: 0.1237 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07518\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 122s 916ms/step - loss: 4.2805 - f1_score: 0.2653 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07486, saving model to ../data/st_model/0_2.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 115s 863ms/step - loss: 14.8867 - f1_score: 0.1242 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07486\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 115s 863ms/step - loss: 14.8877 - f1_score: 0.1241 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07486\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 115s 863ms/step - loss: 14.8833 - f1_score: 0.1246 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07486\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 124s 931ms/step - loss: 8.1620 - f1_score: 0.2112 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07697, saving model to ../data/st_model/1_2.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 117s 876ms/step - loss: 14.8937 - f1_score: 0.1234 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07697\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 116s 876ms/step - loss: 14.8925 - f1_score: 0.1236 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07697\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 116s 874ms/step - loss: 14.8951 - f1_score: 0.1233 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07697\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 126s 946ms/step - loss: 6.4982 - f1_score: 0.2407 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07645, saving model to ../data/st_model/2_2.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 118s 886ms/step - loss: 14.8955 - f1_score: 0.1232 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07645\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 118s 886ms/step - loss: 14.8992 - f1_score: 0.1228 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07645\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 118s 886ms/step - loss: 14.8977 - f1_score: 0.1230 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07645\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 128s 961ms/step - loss: 0.1795 - f1_score: 0.3513 - val_loss: 0.0854 - val_f1_score: 0.5746\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.57462, saving model to ../data/st_model/3_2.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 120s 902ms/step - loss: 7.3533 - f1_score: 0.3032 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.57462\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 120s 902ms/step - loss: 14.8867 - f1_score: 0.1242 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.57462\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 120s 900ms/step - loss: 14.8921 - f1_score: 0.1236 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.57462\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 129s 971ms/step - loss: 9.1577 - f1_score: 0.1614 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07518, saving model to ../data/st_model/4_2.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 121s 908ms/step - loss: 14.8925 - f1_score: 0.1236 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07518\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 121s 909ms/step - loss: 14.8878 - f1_score: 0.1241 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07518\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 121s 909ms/step - loss: 14.8917 - f1_score: 0.1237 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07518\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 131s 987ms/step - loss: 11.0307 - f1_score: 0.1628 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07486, saving model to ../data/st_model/0_3.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 122s 920ms/step - loss: 14.8893 - f1_score: 0.1239 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07486\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 122s 919ms/step - loss: 14.8891 - f1_score: 0.1239 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07486\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 122s 919ms/step - loss: 14.8905 - f1_score: 0.1238 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07486\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 133s 1000ms/step - loss: 11.1497 - f1_score: 0.1589 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07697, saving model to ../data/st_model/1_3.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 124s 931ms/step - loss: 14.8949 - f1_score: 0.1233 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07697\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 124s 931ms/step - loss: 14.8945 - f1_score: 0.1233 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07697\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 124s 931ms/step - loss: 14.8957 - f1_score: 0.1232 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07697\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 135s 1s/step - loss: 0.1742 - f1_score: 0.3510 - val_loss: 0.0858 - val_f1_score: 0.5709\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.57088, saving model to ../data/st_model/2_3.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 126s 945ms/step - loss: 0.1282 - f1_score: 0.4887 - val_loss: 0.0801 - val_f1_score: 0.5846\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.57088 to 0.58455, saving model to ../data/st_model/2_3.att2\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 126s 945ms/step - loss: 0.1120 - f1_score: 0.5233 - val_loss: 0.0756 - val_f1_score: 0.6007\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.58455 to 0.60071, saving model to ../data/st_model/2_3.att2\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 126s 946ms/step - loss: 0.0992 - f1_score: 0.5481 - val_loss: 0.0766 - val_f1_score: 0.6039\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.60071 to 0.60393, saving model to ../data/st_model/2_3.att2\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 126s 946ms/step - loss: 0.0875 - f1_score: 0.5723 - val_loss: 0.0805 - val_f1_score: 0.6055\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.60393 to 0.60547, saving model to ../data/st_model/2_3.att2\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 126s 944ms/step - loss: 0.0780 - f1_score: 0.5991 - val_loss: 0.1463 - val_f1_score: 0.5744\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.60547\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 125s 943ms/step - loss: 14.7842 - f1_score: 0.1266 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.60547\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 125s 943ms/step - loss: 14.8989 - f1_score: 0.1229 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.60547\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 137s 1s/step - loss: 12.2870 - f1_score: 0.1353 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07497, saving model to ../data/st_model/3_3.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 127s 954ms/step - loss: 14.8886 - f1_score: 0.1240 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07497\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 127s 955ms/step - loss: 14.8934 - f1_score: 0.1235 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07497\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 127s 955ms/step - loss: 14.8883 - f1_score: 0.1240 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07497\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 140s 1s/step - loss: 9.5040 - f1_score: 0.1630 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07518, saving model to ../data/st_model/4_3.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 129s 970ms/step - loss: 14.8881 - f1_score: 0.1241 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07518\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 129s 969ms/step - loss: 14.8890 - f1_score: 0.1240 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07518\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 129s 969ms/step - loss: 14.8893 - f1_score: 0.1239 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07518\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 141s 1s/step - loss: 8.3878 - f1_score: 0.1954 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07486, saving model to ../data/st_model/0_4.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 130s 978ms/step - loss: 14.8897 - f1_score: 0.1239 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07486\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 130s 979ms/step - loss: 14.8881 - f1_score: 0.1240 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07486\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 130s 980ms/step - loss: 14.8894 - f1_score: 0.1239 - val_loss: 15.3224 - val_f1_score: 0.0749\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07486\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 142s 1s/step - loss: 10.9534 - f1_score: 0.1507 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07697, saving model to ../data/st_model/1_4.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 131s 987ms/step - loss: 14.8954 - f1_score: 0.1232 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07697\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 131s 987ms/step - loss: 14.8976 - f1_score: 0.1230 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07697\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 131s 987ms/step - loss: 14.8971 - f1_score: 0.1231 - val_loss: 15.3042 - val_f1_score: 0.0770\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07697\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 144s 1s/step - loss: 9.6405 - f1_score: 0.1700 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07645, saving model to ../data/st_model/2_4.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 132s 994ms/step - loss: 14.8922 - f1_score: 0.1236 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07645\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 132s 992ms/step - loss: 14.8947 - f1_score: 0.1233 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07645\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 132s 994ms/step - loss: 14.8956 - f1_score: 0.1232 - val_loss: 15.3087 - val_f1_score: 0.0765\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07645\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 145s 1s/step - loss: 10.2619 - f1_score: 0.1666 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07497, saving model to ../data/st_model/3_4.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 133s 1000ms/step - loss: 14.8896 - f1_score: 0.1239 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07497\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 133s 999ms/step - loss: 14.8857 - f1_score: 0.1243 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07497\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 133s 1000ms/step - loss: 14.8894 - f1_score: 0.1239 - val_loss: 15.3214 - val_f1_score: 0.0750\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07497\n",
      "Epoch 00004: early stopping\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 147s 1s/step - loss: 12.9505 - f1_score: 0.1231 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.07518, saving model to ../data/st_model/4_4.att2\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 134s 1s/step - loss: 14.8913 - f1_score: 0.1237 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00002: val_f1_score did not improve from 0.07518\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 135s 1s/step - loss: 14.8886 - f1_score: 0.1240 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.07518\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 134s 1s/step - loss: 14.8901 - f1_score: 0.1238 - val_loss: 15.3197 - val_f1_score: 0.0752\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.07518\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    get_result_att2(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_cap0(x0,x1):\n",
    "    Embedding_layer0 = Embedding(nb_words + 1, EMBEDDING_DIM, \n",
    "                                weights=[word_embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                name = 'lc_embed0',\n",
    "                                trainable = False,\n",
    "                                )\n",
    "    Embedding_layer1 = Embedding(nb_words + 1, 100, \n",
    "                                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                                name = 'lc_embed1',\n",
    "                                trainable = True,\n",
    "                                )\n",
    "    Embedding_layer2 = Embedding(nb_words1 + 1, EMBEDDING_DIM, \n",
    "                                weights=[word_embedding_matrix1],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH1, \n",
    "                                name = 'lc_embed2',\n",
    "                                trainable = False,\n",
    "                                )\n",
    "    Embedding_layer3 = Embedding(nb_words1 + 1, 100, \n",
    "                                input_length=MAX_SEQUENCE_LENGTH1, \n",
    "                                name = 'lc_embed3',\n",
    "                                trainable = True,\n",
    "                                )\n",
    "    \n",
    "    x = Embedding_layer0(x0)\n",
    "    x01 = Embedding_layer1(x0)\n",
    "    x2 = Embedding_layer2(x1)\n",
    "    x3 = Embedding_layer3(x1)\n",
    "    x01 = SpatialDropout1D(0.5)(x01)\n",
    "    x3 = SpatialDropout1D(0.5)(x3)\n",
    "    \n",
    "    xa0 = Concatenate(axis=2)([x,x01])\n",
    "    xb0 = Concatenate(axis=2)([x2,x3])\n",
    "        \n",
    "    xa = Bidirectional(GRU(128, return_sequences=True,dropout=0.5))(xa0)    \n",
    "    xb = Bidirectional(GRU(128, return_sequences=True,dropout=0.5))(xb0)\n",
    "\n",
    "    xa1 = Capsule(num_capsule=10, dim_capsule=32, routings=5,share_weights=True)(xa)\n",
    "    ya = Flatten()(xa1)\n",
    "    \n",
    "    xb1 = Capsule(num_capsule=10, dim_capsule=32, routings=5,share_weights=True)(xb)\n",
    "    yb = Flatten()(xb1)\n",
    "    \n",
    "    ya = Dropout(0.5)(ya)\n",
    "    yb = Dropout(0.5)(yb)\n",
    "    y = Concatenate(axis=1)([ya,yb])\n",
    "    y = Dense(30, kernel_initializer='he_normal', activation='sigmoid')(y)        \n",
    "    return y\n",
    "\n",
    "\n",
    "def get_result_cap0(timei):\n",
    "    xx_cv = []\n",
    "    xx_pre = []\n",
    "    xx_train = []\n",
    "    yy_train = []\n",
    "    early_stopping = EarlyStopping(patience=3,\n",
    "                                   verbose=1,\n",
    "                                   monitor='val_f1_score',\n",
    "                                   mode='max'\n",
    "                                  )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(factor=0.1, \n",
    "                                  patience=2, \n",
    "                                  min_lr=0.00001, \n",
    "                                  verbose=1,\n",
    "                                  monitor='val_f1_score',\n",
    "                                  mode='max',\n",
    "                                 )\n",
    "    foldi = -1\n",
    "    for train_in,test_in in skf.split(train_words,Y):\n",
    "        foldi = foldi+1\n",
    "        X_train = []\n",
    "        X_traina = []\n",
    "        for i in train_in:\n",
    "            X_train.append(train_words[i])\n",
    "            X_traina.append(train.loc[i,'words1'])\n",
    "\n",
    "        X_test = []\n",
    "        for i in test_in:\n",
    "            X_test.append(train_words[i])\n",
    "\n",
    "        X_train1 = []\n",
    "        for i in train_in:\n",
    "            X_train1.append(train_chars[i])\n",
    "\n",
    "        X_test1 = []\n",
    "        for i in test_in:\n",
    "            X_test1.append(train_chars[i])\n",
    "\n",
    "        y_train,y_test = Y[train_in],Y[test_in]\n",
    "\n",
    "        X_test = get_pad_seq(X_test)\n",
    "        X_test1 = get_pad_char_seq(X_test1)\n",
    "\n",
    "        params = {'batch_size': 128,\n",
    "                  'aug':128,\n",
    "                  'shuffle': True}\n",
    "\n",
    "        training_generator = DataGenerator(X_train,X_train1,y_train, **params)\n",
    "\n",
    "        inputxa = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "        inputxb = Input(shape=(MAX_SEQUENCE_LENGTH1,))\n",
    "        outputya = get_model_cap0(inputxa,inputxb)\n",
    "        model1 = Model([inputxa,inputxb], outputya)\n",
    "\n",
    "\n",
    "        model1.compile(loss='binary_crossentropy', \n",
    "                      optimizer=\"nadam\",\n",
    "                      metrics=[f1_score],\n",
    "                     )\n",
    "        filename = st_model+str(foldi)+'_'+str(timei)+'.cap0'\n",
    "        model_checkpoint = ModelCheckpoint(filename,\n",
    "                                       save_best_only=True,\n",
    "                                       verbose=1,\n",
    "                                       monitor='val_f1_score',\n",
    "                                       mode='max'\n",
    "                                      )\n",
    "            \n",
    "        history =model1.fit_generator(generator=training_generator,\n",
    "                        validation_data=[[X_test,X_test1],y_test],\n",
    "                        epochs=100,\n",
    "                        callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                       )    \n",
    "\n",
    "        model1.load_weights(filename)\n",
    "        ttesty = model1.predict([test_X,test_X1],batch_size = 128)\n",
    "        xx_pre.append(ttesty)\n",
    "        xx_cv.append(np.max(history.history['val_f1_score']))\n",
    "\n",
    "        ttesty = model1.predict([X_test,X_test1],batch_size = 128)\n",
    "        xx_train.append(ttesty)\n",
    "        yy_train.append(y_test)\n",
    "    \n",
    "    s = 0\n",
    "    for i in xx_pre:\n",
    "        s = s+i\n",
    "    s = s/5\n",
    "    \n",
    "    cols = []\n",
    "    for j in range(30):\n",
    "        cols.append(str(j))\n",
    "    res = pd.DataFrame(s,columns=cols)\n",
    "    res.to_csv(st_csv+str(foldi)+'_'+str(timei)+'_cap0.csv',index=None)\n",
    "    \n",
    "    train_res = np.concatenate(xx_train, axis=0)\n",
    "    train_yy = np.concatenate(yy_train, axis=0)\n",
    "    \n",
    "    train_res = pd.DataFrame(train_res,columns=cols)\n",
    "    train_res.to_csv(st_csv+str(foldi)+'_'+str(timei)+'_cap0_train.csv',index=None)\n",
    "    \n",
    "    #train_yy = pd.DataFrame(train_yy,columns=cols)\n",
    "    #train_yy.to_csv(st_csv+str(foldi)+'_'+str(timei)+'_cap0_train_yy.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "66/66 [==============================] - 60s 904ms/step - loss: 0.2177 - f1_score: 0.0300 - val_loss: 0.1434 - val_f1_score: 0.0151\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.01509, saving model to ../data/st_model/0_0.cap0\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 48s 726ms/step - loss: 0.1608 - f1_score: 0.3683 - val_loss: 0.0996 - val_f1_score: 0.5154\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.01509 to 0.51536, saving model to ../data/st_model/0_0.cap0\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 48s 723ms/step - loss: 0.1293 - f1_score: 0.5000 - val_loss: 0.0822 - val_f1_score: 0.5659\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.51536 to 0.56590, saving model to ../data/st_model/0_0.cap0\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 48s 725ms/step - loss: 0.1158 - f1_score: 0.5296 - val_loss: 0.0756 - val_f1_score: 0.5930\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.56590 to 0.59298, saving model to ../data/st_model/0_0.cap0\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 48s 725ms/step - loss: 0.1094 - f1_score: 0.5474 - val_loss: 0.0731 - val_f1_score: 0.6036\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.59298 to 0.60363, saving model to ../data/st_model/0_0.cap0\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 48s 724ms/step - loss: 0.1042 - f1_score: 0.5732 - val_loss: 0.0736 - val_f1_score: 0.6077\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.60363 to 0.60767, saving model to ../data/st_model/0_0.cap0\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 48s 723ms/step - loss: 0.1007 - f1_score: 0.5828 - val_loss: 0.0715 - val_f1_score: 0.6197\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.60767 to 0.61975, saving model to ../data/st_model/0_0.cap0\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 48s 723ms/step - loss: 0.0982 - f1_score: 0.5926 - val_loss: 0.0705 - val_f1_score: 0.6186\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.61975\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 48s 727ms/step - loss: 0.0933 - f1_score: 0.6184 - val_loss: 0.0704 - val_f1_score: 0.6072\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.61975\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 48s 730ms/step - loss: 0.0885 - f1_score: 0.6407 - val_loss: 0.0699 - val_f1_score: 0.6148\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.61975\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/100\n",
      "66/66 [==============================] - 60s 903ms/step - loss: 0.2159 - f1_score: 0.0290 - val_loss: 0.1403 - val_f1_score: 0.2597\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.25973, saving model to ../data/st_model/1_0.cap0\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 48s 722ms/step - loss: 0.1558 - f1_score: 0.4034 - val_loss: 0.1024 - val_f1_score: 0.5269\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.25973 to 0.52693, saving model to ../data/st_model/1_0.cap0\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 48s 727ms/step - loss: 0.1256 - f1_score: 0.5066 - val_loss: 0.0877 - val_f1_score: 0.5589\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.52693 to 0.55888, saving model to ../data/st_model/1_0.cap0\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 48s 721ms/step - loss: 0.1136 - f1_score: 0.5385 - val_loss: 0.0819 - val_f1_score: 0.5797\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.55888 to 0.57967, saving model to ../data/st_model/1_0.cap0\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 48s 727ms/step - loss: 0.1069 - f1_score: 0.5606 - val_loss: 0.0783 - val_f1_score: 0.5758\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.57967\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 48s 722ms/step - loss: 0.1038 - f1_score: 0.5741 - val_loss: 0.0782 - val_f1_score: 0.5966\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.57967 to 0.59658, saving model to ../data/st_model/1_0.cap0\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 48s 725ms/step - loss: 0.1006 - f1_score: 0.5882 - val_loss: 0.0751 - val_f1_score: 0.5791\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.59658\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 48s 723ms/step - loss: 0.0964 - f1_score: 0.6063 - val_loss: 0.0764 - val_f1_score: 0.5957\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.59658\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 48s 722ms/step - loss: 0.0929 - f1_score: 0.6208 - val_loss: 0.0746 - val_f1_score: 0.5899\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.59658\n",
      "Epoch 00009: early stopping\n",
      "Epoch 1/100\n",
      "66/66 [==============================] - 60s 904ms/step - loss: 0.2153 - f1_score: 0.0296 - val_loss: 0.1490 - val_f1_score: 0.2388\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.23876, saving model to ../data/st_model/2_0.cap0\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 48s 720ms/step - loss: 0.1589 - f1_score: 0.3823 - val_loss: 0.1037 - val_f1_score: 0.5106\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.23876 to 0.51062, saving model to ../data/st_model/2_0.cap0\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 48s 725ms/step - loss: 0.1278 - f1_score: 0.5035 - val_loss: 0.0846 - val_f1_score: 0.5705\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.51062 to 0.57052, saving model to ../data/st_model/2_0.cap0\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 47s 718ms/step - loss: 0.1145 - f1_score: 0.5327 - val_loss: 0.0782 - val_f1_score: 0.5753\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.57052 to 0.57529, saving model to ../data/st_model/2_0.cap0\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 47s 719ms/step - loss: 0.1075 - f1_score: 0.5545 - val_loss: 0.0765 - val_f1_score: 0.6064\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.57529 to 0.60644, saving model to ../data/st_model/2_0.cap0\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 48s 721ms/step - loss: 0.1041 - f1_score: 0.5697 - val_loss: 0.0740 - val_f1_score: 0.6024\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.60644\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 48s 724ms/step - loss: 0.1006 - f1_score: 0.5800 - val_loss: 0.0731 - val_f1_score: 0.6010\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.60644\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 48s 723ms/step - loss: 0.0975 - f1_score: 0.5949 - val_loss: 0.0726 - val_f1_score: 0.6081\n",
      "\n",
      "Epoch 00008: val_f1_score improved from 0.60644 to 0.60814, saving model to ../data/st_model/2_0.cap0\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 48s 725ms/step - loss: 0.0968 - f1_score: 0.5991 - val_loss: 0.0726 - val_f1_score: 0.6110\n",
      "\n",
      "Epoch 00009: val_f1_score improved from 0.60814 to 0.61105, saving model to ../data/st_model/2_0.cap0\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 48s 724ms/step - loss: 0.0949 - f1_score: 0.6045 - val_loss: 0.0726 - val_f1_score: 0.6098\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.61105\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 48s 721ms/step - loss: 0.0942 - f1_score: 0.6124 - val_loss: 0.0726 - val_f1_score: 0.6092\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.61105\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 48s 721ms/step - loss: 0.0941 - f1_score: 0.6085 - val_loss: 0.0724 - val_f1_score: 0.6110\n",
      "\n",
      "Epoch 00012: val_f1_score did not improve from 0.61105\n",
      "Epoch 00012: early stopping\n",
      "Epoch 1/100\n",
      "66/66 [==============================] - 61s 920ms/step - loss: 0.2147 - f1_score: 0.0345 - val_loss: 0.1354 - val_f1_score: 0.2900\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.28999, saving model to ../data/st_model/3_0.cap0\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 47s 719ms/step - loss: 0.1603 - f1_score: 0.3871 - val_loss: 0.1005 - val_f1_score: 0.4769\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.28999 to 0.47693, saving model to ../data/st_model/3_0.cap0\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 47s 718ms/step - loss: 0.1317 - f1_score: 0.4860 - val_loss: 0.0817 - val_f1_score: 0.5885\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.47693 to 0.58853, saving model to ../data/st_model/3_0.cap0\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 48s 720ms/step - loss: 0.1163 - f1_score: 0.5314 - val_loss: 0.0761 - val_f1_score: 0.6141\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.58853 to 0.61407, saving model to ../data/st_model/3_0.cap0\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 47s 716ms/step - loss: 0.1095 - f1_score: 0.5524 - val_loss: 0.0722 - val_f1_score: 0.5991\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.61407\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 48s 720ms/step - loss: 0.1053 - f1_score: 0.5662 - val_loss: 0.0716 - val_f1_score: 0.6099\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.61407\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 48s 723ms/step - loss: 0.1017 - f1_score: 0.5805 - val_loss: 0.0700 - val_f1_score: 0.6321\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.61407 to 0.63209, saving model to ../data/st_model/3_0.cap0\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 48s 720ms/step - loss: 0.1010 - f1_score: 0.5831 - val_loss: 0.0699 - val_f1_score: 0.6360\n",
      "\n",
      "Epoch 00008: val_f1_score improved from 0.63209 to 0.63597, saving model to ../data/st_model/3_0.cap0\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 47s 718ms/step - loss: 0.0994 - f1_score: 0.5906 - val_loss: 0.0692 - val_f1_score: 0.6349\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.63597\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 48s 720ms/step - loss: 0.0987 - f1_score: 0.5924 - val_loss: 0.0691 - val_f1_score: 0.6300\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.63597\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 48s 721ms/step - loss: 0.0976 - f1_score: 0.5961 - val_loss: 0.0691 - val_f1_score: 0.6301\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.63597\n",
      "Epoch 00011: early stopping\n",
      "Epoch 1/100\n",
      "66/66 [==============================] - 61s 926ms/step - loss: 0.2150 - f1_score: 0.0374 - val_loss: 0.1344 - val_f1_score: 0.3573\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.35730, saving model to ../data/st_model/4_0.cap0\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 48s 724ms/step - loss: 0.1557 - f1_score: 0.4140 - val_loss: 0.1006 - val_f1_score: 0.4868\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.35730 to 0.48683, saving model to ../data/st_model/4_0.cap0\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 48s 723ms/step - loss: 0.1281 - f1_score: 0.5010 - val_loss: 0.0852 - val_f1_score: 0.5617\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.48683 to 0.56167, saving model to ../data/st_model/4_0.cap0\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 48s 725ms/step - loss: 0.1147 - f1_score: 0.5390 - val_loss: 0.0791 - val_f1_score: 0.5724\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.56167 to 0.57243, saving model to ../data/st_model/4_0.cap0\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 48s 727ms/step - loss: 0.1079 - f1_score: 0.5577 - val_loss: 0.0756 - val_f1_score: 0.5911\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.57243 to 0.59109, saving model to ../data/st_model/4_0.cap0\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 48s 725ms/step - loss: 0.1035 - f1_score: 0.5749 - val_loss: 0.0741 - val_f1_score: 0.5784\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.59109\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 48s 723ms/step - loss: 0.1006 - f1_score: 0.5861 - val_loss: 0.0732 - val_f1_score: 0.6111\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.59109 to 0.61106, saving model to ../data/st_model/4_0.cap0\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 48s 727ms/step - loss: 0.0964 - f1_score: 0.6023 - val_loss: 0.0718 - val_f1_score: 0.5972\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.61106\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 48s 728ms/step - loss: 0.0942 - f1_score: 0.6170 - val_loss: 0.0722 - val_f1_score: 0.6134\n",
      "\n",
      "Epoch 00009: val_f1_score improved from 0.61106 to 0.61343, saving model to ../data/st_model/4_0.cap0\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 48s 730ms/step - loss: 0.0897 - f1_score: 0.6328 - val_loss: 0.0724 - val_f1_score: 0.6092\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.61343\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 48s 729ms/step - loss: 0.0861 - f1_score: 0.6540 - val_loss: 0.0756 - val_f1_score: 0.6088\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.61343\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 48s 727ms/step - loss: 0.0809 - f1_score: 0.6795 - val_loss: 0.0731 - val_f1_score: 0.6023\n",
      "\n",
      "Epoch 00012: val_f1_score did not improve from 0.61343\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    get_result_cap0(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加权平均\n",
    "s = 0\n",
    "for i in range(5):\n",
    "    fn = st_csv+'4_'+str(i)+'_att0.csv'\n",
    "    a = pd.read_csv(fn)\n",
    "    s = s+np.array(a.values)*0.85\n",
    "    \n",
    "    fn = st_csv+'4_'+str(i)+'_att1.csv'\n",
    "    a = pd.read_csv(fn)\n",
    "    s = s+np.array(a.values)*0.05\n",
    "    \n",
    "    fn = st_csv+'4_'+str(i)+'_cap0.csv'\n",
    "    a = pd.read_csv(fn)\n",
    "    s = s+np.array(a.values)*0.1\n",
    "    \n",
    "testy = s/5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#瞎写的stacking，比平均要差一点点\n",
    "new_test = np.zeros((test_X.shape[0],30*15))\n",
    "new_train = np.zeros((len(train),30*15))\n",
    "new_trainy = np.zeros((len(train),30))\n",
    "\n",
    "for i in range(5):\n",
    "    fn = st_csv+'4_'+str(i)+'_att0.csv'\n",
    "    testf = pd.read_csv(fn)\n",
    "    new_test[:,30*i:30*(i+1)] = testf.values\n",
    "    fn = st_csv+'4_'+str(i)+'_att0_train.csv'\n",
    "    trainf = pd.read_csv(fn)\n",
    "    new_train[:,30*i:30*(i+1)] = trainf.values\n",
    "    \n",
    "for i in range(5):\n",
    "    fn = st_csv+'4_'+str(i)+'_att1.csv'\n",
    "    a = pd.read_csv(fn)\n",
    "    new_test[:,30*5+30*i:30*5+30*(i+1)] = a.values\n",
    "    \n",
    "    fn = st_csv+'4_'+str(i)+'_att1_train.csv'\n",
    "    a = pd.read_csv(fn)\n",
    "    new_train[:,30*5+30*i:30*5+30*(i+1)] = a.values\n",
    "    \n",
    "    fn = st_csv+'4_'+str(i)+'_cap0.csv'\n",
    "    a = pd.read_csv(fn)\n",
    "    new_test[:,30*10+30*i:30*10+30*(i+1)] = a.values\n",
    "    \n",
    "    fn = st_csv+'4_'+str(i)+'_cap0_train.csv'\n",
    "    a = pd.read_csv(fn)\n",
    "    new_train[:,30*10+30*i:30*10+30*(i+1)] = a.values\n",
    "    \n",
    "\n",
    "fn = st_csv+'4_0_att0_train_yy.csv'\n",
    "trainfyy = pd.read_csv(fn)\n",
    "new_trainy = trainfyy.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st_stack = '../data/st_stack/'\n",
    "if not os.path.exists(st_stack):\n",
    "    os.mkdir(st_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_57 (InputLayer)        (None, 450)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1024)              461824    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 1024)              1024      \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "p_re_lu_2 (PReLU)            (None, 1024)              1024      \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "p_re_lu_3 (PReLU)            (None, 1024)              1024      \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 1024)              1024      \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 1024)              1024      \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 30)                30750     \n",
      "=================================================================\n",
      "Total params: 4,716,574\n",
      "Trainable params: 4,706,334\n",
      "Non-trainable params: 10,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def stackingmodel(x):\n",
    "    x = Dense(1024, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = PReLU()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    for i in range(4):\n",
    "        x = Dense(1024, kernel_initializer='he_normal')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = PReLU()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "    y = Dense(30, kernel_initializer='he_normal', activation='sigmoid')(x)      \n",
    "    return y\n",
    "inputx = Input(shape=(new_train.shape[1],))\n",
    "outputy = stackingmodel(inputx)\n",
    "stackmodel = Model(inputx, outputy)\n",
    "stackmodel.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8523 samples, validate on 2131 samples\n",
      "Epoch 1/100\n",
      "8523/8523 [==============================] - 13s 1ms/step - loss: 0.1674 - f1_score: 0.3979 - val_loss: 0.1137 - val_f1_score: 0.5680\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.56802, saving model to ../data/st_stack/0.stacking\n",
      "Epoch 2/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0914 - f1_score: 0.5342 - val_loss: 0.0924 - val_f1_score: 0.6016\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.56802 to 0.60164, saving model to ../data/st_stack/0.stacking\n",
      "Epoch 3/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0870 - f1_score: 0.5470 - val_loss: 0.0852 - val_f1_score: 0.6035\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.60164 to 0.60347, saving model to ../data/st_stack/0.stacking\n",
      "Epoch 4/100\n",
      "8523/8523 [==============================] - 1s 135us/step - loss: 0.0841 - f1_score: 0.5478 - val_loss: 0.0801 - val_f1_score: 0.6197\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.60347 to 0.61974, saving model to ../data/st_stack/0.stacking\n",
      "Epoch 5/100\n",
      "8523/8523 [==============================] - 1s 136us/step - loss: 0.0816 - f1_score: 0.5599 - val_loss: 0.0802 - val_f1_score: 0.6166\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.61974\n",
      "Epoch 6/100\n",
      "8523/8523 [==============================] - 1s 135us/step - loss: 0.0797 - f1_score: 0.5695 - val_loss: 0.0764 - val_f1_score: 0.6220\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.61974 to 0.62203, saving model to ../data/st_stack/0.stacking\n",
      "Epoch 7/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0779 - f1_score: 0.5737 - val_loss: 0.0746 - val_f1_score: 0.6297\n",
      "\n",
      "Epoch 00007: val_f1_score improved from 0.62203 to 0.62972, saving model to ../data/st_stack/0.stacking\n",
      "Epoch 8/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0770 - f1_score: 0.5759 - val_loss: 0.0739 - val_f1_score: 0.6294\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.62972\n",
      "Epoch 9/100\n",
      "8523/8523 [==============================] - 1s 140us/step - loss: 0.0758 - f1_score: 0.5835 - val_loss: 0.0730 - val_f1_score: 0.6276\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.62972\n",
      "Epoch 10/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0736 - f1_score: 0.5888 - val_loss: 0.0729 - val_f1_score: 0.6270\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.62972\n",
      "Epoch 11/100\n",
      "8523/8523 [==============================] - 1s 136us/step - loss: 0.0735 - f1_score: 0.5904 - val_loss: 0.0723 - val_f1_score: 0.6311\n",
      "\n",
      "Epoch 00011: val_f1_score improved from 0.62972 to 0.63114, saving model to ../data/st_stack/0.stacking\n",
      "Epoch 12/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0723 - f1_score: 0.5958 - val_loss: 0.0723 - val_f1_score: 0.6263\n",
      "\n",
      "Epoch 00012: val_f1_score did not improve from 0.63114\n",
      "Epoch 13/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0720 - f1_score: 0.5946 - val_loss: 0.0708 - val_f1_score: 0.6276\n",
      "\n",
      "Epoch 00013: val_f1_score did not improve from 0.63114\n",
      "Epoch 14/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0711 - f1_score: 0.6016 - val_loss: 0.0711 - val_f1_score: 0.6301\n",
      "\n",
      "Epoch 00014: val_f1_score did not improve from 0.63114\n",
      "Epoch 15/100\n",
      "8523/8523 [==============================] - 1s 135us/step - loss: 0.0706 - f1_score: 0.6018 - val_loss: 0.0710 - val_f1_score: 0.6339\n",
      "\n",
      "Epoch 00015: val_f1_score improved from 0.63114 to 0.63389, saving model to ../data/st_stack/0.stacking\n",
      "Epoch 16/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0701 - f1_score: 0.6071 - val_loss: 0.0709 - val_f1_score: 0.6285\n",
      "\n",
      "Epoch 00016: val_f1_score did not improve from 0.63389\n",
      "Epoch 17/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0690 - f1_score: 0.6124 - val_loss: 0.0696 - val_f1_score: 0.6304\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.63389\n",
      "Epoch 18/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0688 - f1_score: 0.6120 - val_loss: 0.0704 - val_f1_score: 0.6282\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.63389\n",
      "Epoch 19/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0684 - f1_score: 0.6075 - val_loss: 0.0704 - val_f1_score: 0.6275\n",
      "\n",
      "Epoch 00019: val_f1_score did not improve from 0.63389\n",
      "Epoch 20/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0677 - f1_score: 0.6175 - val_loss: 0.0703 - val_f1_score: 0.6265\n",
      "\n",
      "Epoch 00020: val_f1_score did not improve from 0.63389\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 21/100\n",
      "8523/8523 [==============================] - 1s 136us/step - loss: 0.0661 - f1_score: 0.6265 - val_loss: 0.0695 - val_f1_score: 0.6310\n",
      "\n",
      "Epoch 00021: val_f1_score did not improve from 0.63389\n",
      "Epoch 22/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0655 - f1_score: 0.6282 - val_loss: 0.0692 - val_f1_score: 0.6332\n",
      "\n",
      "Epoch 00022: val_f1_score did not improve from 0.63389\n",
      "Epoch 23/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0654 - f1_score: 0.6271 - val_loss: 0.0691 - val_f1_score: 0.6276\n",
      "\n",
      "Epoch 00023: val_f1_score did not improve from 0.63389\n",
      "Epoch 24/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0656 - f1_score: 0.6255 - val_loss: 0.0691 - val_f1_score: 0.6311\n",
      "\n",
      "Epoch 00024: val_f1_score did not improve from 0.63389\n",
      "Epoch 25/100\n",
      "8523/8523 [==============================] - 1s 138us/step - loss: 0.0651 - f1_score: 0.6274 - val_loss: 0.0690 - val_f1_score: 0.6311\n",
      "\n",
      "Epoch 00025: val_f1_score did not improve from 0.63389\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00025: early stopping\n",
      "Train on 8523 samples, validate on 2131 samples\n",
      "Epoch 1/100\n",
      "8523/8523 [==============================] - 12s 1ms/step - loss: 0.1718 - f1_score: 0.3991 - val_loss: 0.1191 - val_f1_score: 0.5570\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.55702, saving model to ../data/st_stack/1.stacking\n",
      "Epoch 2/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0923 - f1_score: 0.5361 - val_loss: 0.0934 - val_f1_score: 0.5959\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.55702 to 0.59594, saving model to ../data/st_stack/1.stacking\n",
      "Epoch 3/100\n",
      "8523/8523 [==============================] - 1s 138us/step - loss: 0.0874 - f1_score: 0.5454 - val_loss: 0.0872 - val_f1_score: 0.5935\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.59594\n",
      "Epoch 4/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0840 - f1_score: 0.5517 - val_loss: 0.0850 - val_f1_score: 0.5941\n",
      "\n",
      "Epoch 00004: val_f1_score did not improve from 0.59594\n",
      "Epoch 5/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0804 - f1_score: 0.5696 - val_loss: 0.0791 - val_f1_score: 0.6213\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.59594 to 0.62135, saving model to ../data/st_stack/1.stacking\n",
      "Epoch 6/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0796 - f1_score: 0.5705 - val_loss: 0.0764 - val_f1_score: 0.6247\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.62135 to 0.62465, saving model to ../data/st_stack/1.stacking\n",
      "Epoch 7/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0777 - f1_score: 0.5737 - val_loss: 0.0762 - val_f1_score: 0.6237\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.62465\n",
      "Epoch 8/100\n",
      "8523/8523 [==============================] - 1s 136us/step - loss: 0.0763 - f1_score: 0.5825 - val_loss: 0.0751 - val_f1_score: 0.6211\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.62465\n",
      "Epoch 9/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0754 - f1_score: 0.5827 - val_loss: 0.0737 - val_f1_score: 0.6253\n",
      "\n",
      "Epoch 00009: val_f1_score improved from 0.62465 to 0.62528, saving model to ../data/st_stack/1.stacking\n",
      "Epoch 10/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0739 - f1_score: 0.5856 - val_loss: 0.0735 - val_f1_score: 0.6339\n",
      "\n",
      "Epoch 00010: val_f1_score improved from 0.62528 to 0.63393, saving model to ../data/st_stack/1.stacking\n",
      "Epoch 11/100\n",
      "8523/8523 [==============================] - 1s 136us/step - loss: 0.0733 - f1_score: 0.5947 - val_loss: 0.0722 - val_f1_score: 0.6331\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.63393\n",
      "Epoch 12/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0723 - f1_score: 0.5966 - val_loss: 0.0723 - val_f1_score: 0.6361\n",
      "\n",
      "Epoch 00012: val_f1_score improved from 0.63393 to 0.63614, saving model to ../data/st_stack/1.stacking\n",
      "Epoch 13/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0722 - f1_score: 0.5978 - val_loss: 0.0718 - val_f1_score: 0.6263\n",
      "\n",
      "Epoch 00013: val_f1_score did not improve from 0.63614\n",
      "Epoch 14/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0711 - f1_score: 0.6020 - val_loss: 0.0710 - val_f1_score: 0.6325\n",
      "\n",
      "Epoch 00014: val_f1_score did not improve from 0.63614\n",
      "Epoch 15/100\n",
      "8523/8523 [==============================] - 1s 140us/step - loss: 0.0706 - f1_score: 0.6047 - val_loss: 0.0714 - val_f1_score: 0.6317\n",
      "\n",
      "Epoch 00015: val_f1_score did not improve from 0.63614\n",
      "Epoch 16/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0700 - f1_score: 0.6026 - val_loss: 0.0714 - val_f1_score: 0.6201\n",
      "\n",
      "Epoch 00016: val_f1_score did not improve from 0.63614\n",
      "Epoch 17/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0696 - f1_score: 0.6062 - val_loss: 0.0714 - val_f1_score: 0.6189\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.63614\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 18/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0684 - f1_score: 0.6136 - val_loss: 0.0696 - val_f1_score: 0.6319\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.63614\n",
      "Epoch 19/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0677 - f1_score: 0.6146 - val_loss: 0.0692 - val_f1_score: 0.6334\n",
      "\n",
      "Epoch 00019: val_f1_score did not improve from 0.63614\n",
      "Epoch 20/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0671 - f1_score: 0.6154 - val_loss: 0.0692 - val_f1_score: 0.6313\n",
      "\n",
      "Epoch 00020: val_f1_score did not improve from 0.63614\n",
      "Epoch 21/100\n",
      "8523/8523 [==============================] - 1s 136us/step - loss: 0.0667 - f1_score: 0.6208 - val_loss: 0.0690 - val_f1_score: 0.6348\n",
      "\n",
      "Epoch 00021: val_f1_score did not improve from 0.63614\n",
      "Epoch 22/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0668 - f1_score: 0.6276 - val_loss: 0.0690 - val_f1_score: 0.6338\n",
      "\n",
      "Epoch 00022: val_f1_score did not improve from 0.63614\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00022: early stopping\n",
      "Train on 8523 samples, validate on 2131 samples\n",
      "Epoch 1/100\n",
      "8523/8523 [==============================] - 13s 1ms/step - loss: 0.1667 - f1_score: 0.3940 - val_loss: 0.1123 - val_f1_score: 0.5584\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.55842, saving model to ../data/st_stack/2.stacking\n",
      "Epoch 2/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0923 - f1_score: 0.5243 - val_loss: 0.0907 - val_f1_score: 0.6125\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.55842 to 0.61253, saving model to ../data/st_stack/2.stacking\n",
      "Epoch 3/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0869 - f1_score: 0.5392 - val_loss: 0.0856 - val_f1_score: 0.6053\n",
      "\n",
      "Epoch 00003: val_f1_score did not improve from 0.61253\n",
      "Epoch 4/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0838 - f1_score: 0.5526 - val_loss: 0.0790 - val_f1_score: 0.6341\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.61253 to 0.63410, saving model to ../data/st_stack/2.stacking\n",
      "Epoch 5/100\n",
      "8523/8523 [==============================] - 1s 141us/step - loss: 0.0814 - f1_score: 0.5533 - val_loss: 0.0784 - val_f1_score: 0.6313\n",
      "\n",
      "Epoch 00005: val_f1_score did not improve from 0.63410\n",
      "Epoch 6/100\n",
      "8523/8523 [==============================] - 1s 138us/step - loss: 0.0801 - f1_score: 0.5628 - val_loss: 0.0756 - val_f1_score: 0.6308\n",
      "\n",
      "Epoch 00006: val_f1_score did not improve from 0.63410\n",
      "Epoch 7/100\n",
      "8523/8523 [==============================] - 1s 140us/step - loss: 0.0786 - f1_score: 0.5662 - val_loss: 0.0747 - val_f1_score: 0.6318\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.63410\n",
      "Epoch 8/100\n",
      "8523/8523 [==============================] - 1s 138us/step - loss: 0.0770 - f1_score: 0.5719 - val_loss: 0.0734 - val_f1_score: 0.6306\n",
      "\n",
      "Epoch 00008: val_f1_score did not improve from 0.63410\n",
      "Epoch 9/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0760 - f1_score: 0.5705 - val_loss: 0.0708 - val_f1_score: 0.6443\n",
      "\n",
      "Epoch 00009: val_f1_score improved from 0.63410 to 0.64430, saving model to ../data/st_stack/2.stacking\n",
      "Epoch 10/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0746 - f1_score: 0.5775 - val_loss: 0.0703 - val_f1_score: 0.6388\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.64430\n",
      "Epoch 11/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0735 - f1_score: 0.5880 - val_loss: 0.0701 - val_f1_score: 0.6309\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.64430\n",
      "Epoch 12/100\n",
      "8523/8523 [==============================] - 1s 138us/step - loss: 0.0730 - f1_score: 0.5898 - val_loss: 0.0701 - val_f1_score: 0.6360\n",
      "\n",
      "Epoch 00012: val_f1_score did not improve from 0.64430\n",
      "Epoch 13/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0722 - f1_score: 0.5904 - val_loss: 0.0704 - val_f1_score: 0.6536\n",
      "\n",
      "Epoch 00013: val_f1_score improved from 0.64430 to 0.65360, saving model to ../data/st_stack/2.stacking\n",
      "Epoch 14/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0714 - f1_score: 0.5980 - val_loss: 0.0699 - val_f1_score: 0.6464\n",
      "\n",
      "Epoch 00014: val_f1_score did not improve from 0.65360\n",
      "Epoch 15/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0713 - f1_score: 0.5992 - val_loss: 0.0682 - val_f1_score: 0.6457\n",
      "\n",
      "Epoch 00015: val_f1_score did not improve from 0.65360\n",
      "Epoch 16/100\n",
      "8523/8523 [==============================] - 1s 138us/step - loss: 0.0709 - f1_score: 0.6003 - val_loss: 0.0678 - val_f1_score: 0.6417\n",
      "\n",
      "Epoch 00016: val_f1_score did not improve from 0.65360\n",
      "Epoch 17/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0703 - f1_score: 0.6062 - val_loss: 0.0677 - val_f1_score: 0.6439\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.65360\n",
      "Epoch 18/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0692 - f1_score: 0.6064 - val_loss: 0.0689 - val_f1_score: 0.6332\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.65360\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/100\n",
      "8523/8523 [==============================] - 1s 141us/step - loss: 0.0675 - f1_score: 0.6100 - val_loss: 0.0670 - val_f1_score: 0.6454\n",
      "\n",
      "Epoch 00019: val_f1_score did not improve from 0.65360\n",
      "Epoch 20/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0674 - f1_score: 0.6154 - val_loss: 0.0668 - val_f1_score: 0.6480\n",
      "\n",
      "Epoch 00020: val_f1_score did not improve from 0.65360\n",
      "Epoch 21/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0667 - f1_score: 0.6160 - val_loss: 0.0668 - val_f1_score: 0.6489\n",
      "\n",
      "Epoch 00021: val_f1_score did not improve from 0.65360\n",
      "Epoch 22/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0668 - f1_score: 0.6173 - val_loss: 0.0668 - val_f1_score: 0.6470\n",
      "\n",
      "Epoch 00022: val_f1_score did not improve from 0.65360\n",
      "Epoch 23/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0666 - f1_score: 0.6225 - val_loss: 0.0667 - val_f1_score: 0.6508\n",
      "\n",
      "Epoch 00023: val_f1_score did not improve from 0.65360\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00023: early stopping\n",
      "Train on 8523 samples, validate on 2131 samples\n",
      "Epoch 1/100\n",
      "8523/8523 [==============================] - 14s 2ms/step - loss: 0.1693 - f1_score: 0.3905 - val_loss: 0.1111 - val_f1_score: 0.5674\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.56739, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 2/100\n",
      "8523/8523 [==============================] - 1s 143us/step - loss: 0.0916 - f1_score: 0.5372 - val_loss: 0.0923 - val_f1_score: 0.5803\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.56739 to 0.58026, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 3/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0870 - f1_score: 0.5441 - val_loss: 0.0843 - val_f1_score: 0.5964\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.58026 to 0.59636, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 4/100\n",
      "8523/8523 [==============================] - 1s 141us/step - loss: 0.0836 - f1_score: 0.5515 - val_loss: 0.0807 - val_f1_score: 0.6115\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.59636 to 0.61154, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 5/100\n",
      "8523/8523 [==============================] - 1s 141us/step - loss: 0.0814 - f1_score: 0.5607 - val_loss: 0.0778 - val_f1_score: 0.6127\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.61154 to 0.61272, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 6/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0800 - f1_score: 0.5657 - val_loss: 0.0760 - val_f1_score: 0.6140\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.61272 to 0.61401, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 7/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0783 - f1_score: 0.5729 - val_loss: 0.0756 - val_f1_score: 0.6090\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.61401\n",
      "Epoch 8/100\n",
      "8523/8523 [==============================] - 1s 141us/step - loss: 0.0770 - f1_score: 0.5770 - val_loss: 0.0726 - val_f1_score: 0.6230\n",
      "\n",
      "Epoch 00008: val_f1_score improved from 0.61401 to 0.62296, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 9/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0754 - f1_score: 0.5804 - val_loss: 0.0717 - val_f1_score: 0.6292\n",
      "\n",
      "Epoch 00009: val_f1_score improved from 0.62296 to 0.62919, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 10/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0744 - f1_score: 0.5877 - val_loss: 0.0718 - val_f1_score: 0.6323\n",
      "\n",
      "Epoch 00010: val_f1_score improved from 0.62919 to 0.63226, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 11/100\n",
      "8523/8523 [==============================] - 1s 143us/step - loss: 0.0735 - f1_score: 0.5913 - val_loss: 0.0705 - val_f1_score: 0.6325\n",
      "\n",
      "Epoch 00011: val_f1_score improved from 0.63226 to 0.63254, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 12/100\n",
      "8523/8523 [==============================] - 1s 143us/step - loss: 0.0724 - f1_score: 0.5981 - val_loss: 0.0704 - val_f1_score: 0.6301\n",
      "\n",
      "Epoch 00012: val_f1_score did not improve from 0.63254\n",
      "Epoch 13/100\n",
      "8523/8523 [==============================] - 1s 141us/step - loss: 0.0725 - f1_score: 0.5940 - val_loss: 0.0701 - val_f1_score: 0.6346\n",
      "\n",
      "Epoch 00013: val_f1_score improved from 0.63254 to 0.63457, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 14/100\n",
      "8523/8523 [==============================] - 1s 143us/step - loss: 0.0708 - f1_score: 0.6032 - val_loss: 0.0693 - val_f1_score: 0.6294\n",
      "\n",
      "Epoch 00014: val_f1_score did not improve from 0.63457\n",
      "Epoch 15/100\n",
      "8523/8523 [==============================] - 1s 143us/step - loss: 0.0706 - f1_score: 0.6031 - val_loss: 0.0691 - val_f1_score: 0.6263\n",
      "\n",
      "Epoch 00015: val_f1_score did not improve from 0.63457\n",
      "Epoch 16/100\n",
      "8523/8523 [==============================] - 1s 141us/step - loss: 0.0705 - f1_score: 0.6076 - val_loss: 0.0697 - val_f1_score: 0.6345\n",
      "\n",
      "Epoch 00016: val_f1_score did not improve from 0.63457\n",
      "Epoch 17/100\n",
      "8523/8523 [==============================] - 1s 141us/step - loss: 0.0695 - f1_score: 0.6072 - val_loss: 0.0688 - val_f1_score: 0.6329\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.63457\n",
      "Epoch 18/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0692 - f1_score: 0.6105 - val_loss: 0.0695 - val_f1_score: 0.6224\n",
      "\n",
      "Epoch 00018: val_f1_score did not improve from 0.63457\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 19/100\n",
      "8523/8523 [==============================] - 1s 140us/step - loss: 0.0677 - f1_score: 0.6103 - val_loss: 0.0677 - val_f1_score: 0.6357\n",
      "\n",
      "Epoch 00019: val_f1_score improved from 0.63457 to 0.63575, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 20/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0675 - f1_score: 0.6167 - val_loss: 0.0674 - val_f1_score: 0.6370\n",
      "\n",
      "Epoch 00020: val_f1_score improved from 0.63575 to 0.63704, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 21/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0669 - f1_score: 0.6234 - val_loss: 0.0673 - val_f1_score: 0.6385\n",
      "\n",
      "Epoch 00021: val_f1_score improved from 0.63704 to 0.63845, saving model to ../data/st_stack/3.stacking\n",
      "Epoch 22/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0665 - f1_score: 0.6245 - val_loss: 0.0672 - val_f1_score: 0.6361\n",
      "\n",
      "Epoch 00022: val_f1_score did not improve from 0.63845\n",
      "Epoch 23/100\n",
      "8523/8523 [==============================] - 1s 136us/step - loss: 0.0662 - f1_score: 0.6230 - val_loss: 0.0673 - val_f1_score: 0.6358\n",
      "\n",
      "Epoch 00023: val_f1_score did not improve from 0.63845\n",
      "Epoch 24/100\n",
      "8523/8523 [==============================] - 1s 141us/step - loss: 0.0662 - f1_score: 0.6283 - val_loss: 0.0672 - val_f1_score: 0.6347\n",
      "\n",
      "Epoch 00024: val_f1_score did not improve from 0.63845\n",
      "Epoch 25/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0663 - f1_score: 0.6251 - val_loss: 0.0673 - val_f1_score: 0.6339\n",
      "\n",
      "Epoch 00025: val_f1_score did not improve from 0.63845\n",
      "Epoch 26/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0658 - f1_score: 0.6225 - val_loss: 0.0674 - val_f1_score: 0.6366\n",
      "\n",
      "Epoch 00026: val_f1_score did not improve from 0.63845\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 27/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0659 - f1_score: 0.6274 - val_loss: 0.0673 - val_f1_score: 0.6371\n",
      "\n",
      "Epoch 00027: val_f1_score did not improve from 0.63845\n",
      "Epoch 28/100\n",
      "8523/8523 [==============================] - 1s 137us/step - loss: 0.0660 - f1_score: 0.6263 - val_loss: 0.0673 - val_f1_score: 0.6369\n",
      "\n",
      "Epoch 00028: val_f1_score did not improve from 0.63845\n",
      "Epoch 29/100\n",
      "8523/8523 [==============================] - 1s 141us/step - loss: 0.0652 - f1_score: 0.6303 - val_loss: 0.0673 - val_f1_score: 0.6368\n",
      "\n",
      "Epoch 00029: val_f1_score did not improve from 0.63845\n",
      "Epoch 30/100\n",
      "8523/8523 [==============================] - 1s 141us/step - loss: 0.0655 - f1_score: 0.6288 - val_loss: 0.0673 - val_f1_score: 0.6369\n",
      "\n",
      "Epoch 00030: val_f1_score did not improve from 0.63845\n",
      "Epoch 31/100\n",
      "8523/8523 [==============================] - 1s 139us/step - loss: 0.0658 - f1_score: 0.6273 - val_loss: 0.0673 - val_f1_score: 0.6372\n",
      "\n",
      "Epoch 00031: val_f1_score did not improve from 0.63845\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 00031: early stopping\n",
      "Train on 8524 samples, validate on 2130 samples\n",
      "Epoch 1/100\n",
      "8524/8524 [==============================] - 13s 2ms/step - loss: 0.1638 - f1_score: 0.4049 - val_loss: 0.1131 - val_f1_score: 0.5449\n",
      "\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.54494, saving model to ../data/st_stack/4.stacking\n",
      "Epoch 2/100\n",
      "8524/8524 [==============================] - 1s 143us/step - loss: 0.0913 - f1_score: 0.5377 - val_loss: 0.0944 - val_f1_score: 0.5810\n",
      "\n",
      "Epoch 00002: val_f1_score improved from 0.54494 to 0.58101, saving model to ../data/st_stack/4.stacking\n",
      "Epoch 3/100\n",
      "8524/8524 [==============================] - 1s 141us/step - loss: 0.0863 - f1_score: 0.5488 - val_loss: 0.0874 - val_f1_score: 0.5981\n",
      "\n",
      "Epoch 00003: val_f1_score improved from 0.58101 to 0.59805, saving model to ../data/st_stack/4.stacking\n",
      "Epoch 4/100\n",
      "8524/8524 [==============================] - 1s 141us/step - loss: 0.0833 - f1_score: 0.5552 - val_loss: 0.0821 - val_f1_score: 0.6015\n",
      "\n",
      "Epoch 00004: val_f1_score improved from 0.59805 to 0.60149, saving model to ../data/st_stack/4.stacking\n",
      "Epoch 5/100\n",
      "8524/8524 [==============================] - 1s 139us/step - loss: 0.0812 - f1_score: 0.5635 - val_loss: 0.0804 - val_f1_score: 0.6120\n",
      "\n",
      "Epoch 00005: val_f1_score improved from 0.60149 to 0.61205, saving model to ../data/st_stack/4.stacking\n",
      "Epoch 6/100\n",
      "8524/8524 [==============================] - 1s 139us/step - loss: 0.0791 - f1_score: 0.5668 - val_loss: 0.0772 - val_f1_score: 0.6183\n",
      "\n",
      "Epoch 00006: val_f1_score improved from 0.61205 to 0.61835, saving model to ../data/st_stack/4.stacking\n",
      "Epoch 7/100\n",
      "8524/8524 [==============================] - 1s 137us/step - loss: 0.0775 - f1_score: 0.5749 - val_loss: 0.0759 - val_f1_score: 0.6147\n",
      "\n",
      "Epoch 00007: val_f1_score did not improve from 0.61835\n",
      "Epoch 8/100\n",
      "8524/8524 [==============================] - 1s 137us/step - loss: 0.0757 - f1_score: 0.5785 - val_loss: 0.0749 - val_f1_score: 0.6253\n",
      "\n",
      "Epoch 00008: val_f1_score improved from 0.61835 to 0.62532, saving model to ../data/st_stack/4.stacking\n",
      "Epoch 9/100\n",
      "8524/8524 [==============================] - 1s 139us/step - loss: 0.0747 - f1_score: 0.5849 - val_loss: 0.0749 - val_f1_score: 0.6218\n",
      "\n",
      "Epoch 00009: val_f1_score did not improve from 0.62532\n",
      "Epoch 10/100\n",
      "8524/8524 [==============================] - 1s 139us/step - loss: 0.0743 - f1_score: 0.5791 - val_loss: 0.0736 - val_f1_score: 0.6114\n",
      "\n",
      "Epoch 00010: val_f1_score did not improve from 0.62532\n",
      "Epoch 11/100\n",
      "8524/8524 [==============================] - 1s 138us/step - loss: 0.0734 - f1_score: 0.5912 - val_loss: 0.0730 - val_f1_score: 0.6193\n",
      "\n",
      "Epoch 00011: val_f1_score did not improve from 0.62532\n",
      "Epoch 12/100\n",
      "8524/8524 [==============================] - 1s 137us/step - loss: 0.0726 - f1_score: 0.5951 - val_loss: 0.0730 - val_f1_score: 0.6269\n",
      "\n",
      "Epoch 00012: val_f1_score improved from 0.62532 to 0.62687, saving model to ../data/st_stack/4.stacking\n",
      "Epoch 13/100\n",
      "8524/8524 [==============================] - 1s 139us/step - loss: 0.0713 - f1_score: 0.5985 - val_loss: 0.0717 - val_f1_score: 0.6240\n",
      "\n",
      "Epoch 00013: val_f1_score did not improve from 0.62687\n",
      "Epoch 14/100\n",
      "8524/8524 [==============================] - 1s 137us/step - loss: 0.0711 - f1_score: 0.6014 - val_loss: 0.0717 - val_f1_score: 0.6282\n",
      "\n",
      "Epoch 00014: val_f1_score improved from 0.62687 to 0.62815, saving model to ../data/st_stack/4.stacking\n",
      "Epoch 15/100\n",
      "8524/8524 [==============================] - 1s 138us/step - loss: 0.0702 - f1_score: 0.6057 - val_loss: 0.0728 - val_f1_score: 0.6224\n",
      "\n",
      "Epoch 00015: val_f1_score did not improve from 0.62815\n",
      "Epoch 16/100\n",
      "8524/8524 [==============================] - 1s 137us/step - loss: 0.0698 - f1_score: 0.6055 - val_loss: 0.0717 - val_f1_score: 0.6245\n",
      "\n",
      "Epoch 00016: val_f1_score did not improve from 0.62815\n",
      "Epoch 17/100\n",
      "8524/8524 [==============================] - 1s 141us/step - loss: 0.0687 - f1_score: 0.6081 - val_loss: 0.0716 - val_f1_score: 0.6227\n",
      "\n",
      "Epoch 00017: val_f1_score did not improve from 0.62815\n",
      "Epoch 18/100\n",
      "8524/8524 [==============================] - 1s 137us/step - loss: 0.0687 - f1_score: 0.6110 - val_loss: 0.0714 - val_f1_score: 0.6323\n",
      "\n",
      "Epoch 00018: val_f1_score improved from 0.62815 to 0.63231, saving model to ../data/st_stack/4.stacking\n",
      "Epoch 19/100\n",
      "8524/8524 [==============================] - 1s 139us/step - loss: 0.0684 - f1_score: 0.6128 - val_loss: 0.0713 - val_f1_score: 0.6222\n",
      "\n",
      "Epoch 00019: val_f1_score did not improve from 0.63231\n",
      "Epoch 20/100\n",
      "8524/8524 [==============================] - 1s 137us/step - loss: 0.0682 - f1_score: 0.6141 - val_loss: 0.0711 - val_f1_score: 0.6216\n",
      "\n",
      "Epoch 00020: val_f1_score did not improve from 0.63231\n",
      "Epoch 21/100\n",
      "8524/8524 [==============================] - 1s 138us/step - loss: 0.0673 - f1_score: 0.6182 - val_loss: 0.0718 - val_f1_score: 0.6259\n",
      "\n",
      "Epoch 00021: val_f1_score did not improve from 0.63231\n",
      "Epoch 22/100\n",
      "8524/8524 [==============================] - 1s 137us/step - loss: 0.0671 - f1_score: 0.6234 - val_loss: 0.0719 - val_f1_score: 0.6254\n",
      "\n",
      "Epoch 00022: val_f1_score did not improve from 0.63231\n",
      "Epoch 23/100\n",
      "8524/8524 [==============================] - 1s 139us/step - loss: 0.0667 - f1_score: 0.6201 - val_loss: 0.0717 - val_f1_score: 0.6286\n",
      "\n",
      "Epoch 00023: val_f1_score did not improve from 0.63231\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/100\n",
      "8524/8524 [==============================] - 1s 140us/step - loss: 0.0651 - f1_score: 0.6334 - val_loss: 0.0701 - val_f1_score: 0.6272\n",
      "\n",
      "Epoch 00024: val_f1_score did not improve from 0.63231\n",
      "Epoch 25/100\n",
      "8524/8524 [==============================] - 1s 139us/step - loss: 0.0646 - f1_score: 0.6311 - val_loss: 0.0698 - val_f1_score: 0.6262\n",
      "\n",
      "Epoch 00025: val_f1_score did not improve from 0.63231\n",
      "Epoch 26/100\n",
      "8524/8524 [==============================] - 1s 138us/step - loss: 0.0642 - f1_score: 0.6319 - val_loss: 0.0696 - val_f1_score: 0.6238\n",
      "\n",
      "Epoch 00026: val_f1_score did not improve from 0.63231\n",
      "Epoch 27/100\n",
      "8524/8524 [==============================] - 1s 139us/step - loss: 0.0638 - f1_score: 0.6401 - val_loss: 0.0696 - val_f1_score: 0.6271\n",
      "\n",
      "Epoch 00027: val_f1_score did not improve from 0.63231\n",
      "Epoch 28/100\n",
      "8524/8524 [==============================] - 1s 140us/step - loss: 0.0638 - f1_score: 0.6325 - val_loss: 0.0696 - val_f1_score: 0.6275\n",
      "\n",
      "Epoch 00028: val_f1_score did not improve from 0.63231\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00028: early stopping\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "skf1 = KFold(n_splits=N,shuffle=True,random_state=None)\n",
    "xx_cv = []\n",
    "xx_pre = []\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10,\n",
    "                               verbose=1,\n",
    "                               monitor='val_f1_score',\n",
    "                               mode='max'\n",
    "                              )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, \n",
    "                              patience=5, \n",
    "                              min_lr=0.00001, \n",
    "                              verbose=1,\n",
    "                              monitor='val_f1_score',\n",
    "                              mode='max',\n",
    "                             )\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "foldi = -1\n",
    "for train_in,test_in in skf1.split(new_train,new_trainy):\n",
    "    foldi+=1\n",
    "    traina = new_train[train_in]\n",
    "    vala = new_train[test_in]\n",
    "    trainay = new_trainy[train_in]\n",
    "    valay = new_trainy[test_in]\n",
    "    \n",
    "    inputx = Input(shape=(new_train.shape[1],))\n",
    "    outputy = stackingmodel(inputx)\n",
    "    stackmodel = Model(inputx, outputy)\n",
    "    \n",
    "    stackmodel.compile(loss='binary_crossentropy', \n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[f1_score],\n",
    "                 )\n",
    "    filename = st_stack+str(foldi)+'.stacking'\n",
    "    model_checkpoint = ModelCheckpoint(filename,\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=1,\n",
    "                                   monitor='val_f1_score',\n",
    "                                   mode='max'\n",
    "                                  )\n",
    "\n",
    "\n",
    "    \n",
    "    history = stackmodel.fit(traina,trainay,\n",
    "                        validation_data=[vala,valay],\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping, model_checkpoint, reduce_lr])\n",
    "        \n",
    "    stackmodel.load_weights(filename)\n",
    "    ttesty = stackmodel.predict(new_test,batch_size = 128)\n",
    "    xx_pre.append(ttesty)\n",
    "    xx_cv.append(np.max(history.history['val_f1_score']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6388790937156792\n"
     ]
    }
   ],
   "source": [
    "print (np.mean(xx_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6412328714541581\n"
     ]
    }
   ],
   "source": [
    "print (np.mean(xx_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = 0\n",
    "for i in xx_pre:\n",
    "    s = s+i\n",
    "testy = s/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3049\n"
     ]
    }
   ],
   "source": [
    "#调阈值，复赛提交3030左右\n",
    "th = 0.53\n",
    "yy = testy.copy()\n",
    "ans = []\n",
    "for i in range(len(testy)):\n",
    "    mm = np.max(yy[i])\n",
    "    if mm<=th:\n",
    "        x = np.argmax(yy[i])\n",
    "        d = {}\n",
    "        d['content_id'] = test.loc[i,'content_id']\n",
    "        d['subject'] = subject[x//3]\n",
    "        d['sentiment_value'] = sentiment_value[x%3]\n",
    "        ans.append(d)\n",
    "    else:\n",
    "        for j in range(yy.shape[1]):\n",
    "            if yy[i,j]>th:\n",
    "                x = j\n",
    "                d = {}\n",
    "                d['content_id'] = test.loc[i,'content_id']\n",
    "                d['subject'] = subject[x//3]\n",
    "                d['sentiment_value'] = sentiment_value[x%3]\n",
    "                ans.append(d)\n",
    "print (len(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3049, 4)\n"
     ]
    }
   ],
   "source": [
    "ans = pd.DataFrame(ans)\n",
    "ans['sentiment_word'] = None\n",
    "print (ans.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans[['content_id','subject','sentiment_value','sentiment_word']].to_csv('../submit/1111_3049_75.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
